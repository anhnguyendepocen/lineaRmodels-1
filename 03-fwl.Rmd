
# Frisch--Waugh--Lovell theorem

The FWL theorem has two components: it gives a formula for partitioned OLS estimates and shows that residuals from sequential regressions are identical.

Consider the following linear regression 
$$
 \by = \mathbf{X}_1\bbeta_1+\mathbf{X}_2\bbeta_2+ \bs{u}, \label{eq1}
$$
where the response vector $\by$ is $n \times 1$, the vector of errors $\bs{u}$ is a realization from a mean zero random 
variable. The $n \times p$ full-rank design matrix $\mathbf{X}$ can be written as the partitioned 
matrix $(\mathbf{X}_1^\top, \mathbf{X}_2^\top)^\top$ with blocks $\mathbf{X}_1$, an $n \times p_1$ matrix, and $\mathbf{X}_2$, an $n \times p_2$ matrix. Let 
$\hbb_1$ 
and $\hbb_2$ be the ordinary 
least square (OLS) parameter estimates from running this regression. Define the orthogonal projection matrix $\Hmat_\mathbf{X}$ as usual and 
$\Hmat_{\mathbf{X}_i} = \mathbf{X}_i(\mathbf{X}_i^\top\mathbf{X}_i)^{-1}\mathbf{X}_i^\top$ for $i=1, 2$. Similarly, 
define the complementary projection matrices $\Mmat_{\mathbf{X}_1}=\mathbf{I}_n-\Hmat_{\mathbf{X}_1}$ and $\Mmat_{\mathbf{X}_2}=\mathbf{I}_n-\Hmat_{\mathbf{X}_2}$.

```{theorem}
The ordinary least square estimates of $\bbeta_2$ and the residuals from \eqref{eq1} are identical to those obtained by 
running the regression 
$$
 \Mmat_{\mathbf{X}_1}\by = \Mmat_{\mathbf{X}_1}\mathbf{X}_2\bbeta_2 + \text{residuals}. \label{eq2} \
$$
```


```{block2, type = "rmdcaution"}
In general, premultiplying both sides of the regression model by a projection matrix alters the model, so you will get different fitted values and residuals. Similarly, the model 
\[\boldsymbol{Y} = \mX_1 \bbeta_1 + \mX_2\bbeta_2 + \beps\]
is **not** equivalent to 
\[
\mathbf{M}_{\mathbf{X}_1}\boldsymbol{y} = \mathbf{M}_{\mathbf{X}_1}\mX_2 \bbeta_2 + \mathbf{M}_{\mathbf{X}_1}\beps
\]
because $\beps$ is not in $\mathbf{M}_\mathbf{X}$. This is true for the orthogonal decomposition
\[
\mathbf{M}_{\mathbf{X}_1}\boldsymbol{y} = \mathbf{M}_{\mathbf{X}_1}\mX_2 \hat{\bbeta}_2 + \mathbf{M}_{\mathbf{X}_1}\boldsymbol{e}
\]
```

Below is an algebraic proof of the equality of the OLS coefficients. The following material is **optional**. 

```{proof}
The easiest proof uses projection matrices, but we demonstrate the result for OLS coefficients directly.
Consider an invertible $d \times d$ matrix $\mathbf{C}$ and denote its inverse by $\mathbf{D}$; then
$$
\begin{pmatrix} \mathbf{C}_{11} & \mathbf{C}_{12} \\ \mathbf{C}_{21} &\mathbf{C}_{22}
\end{pmatrix}\begin{pmatrix} \mathbf{D}_{11} & \mathbf{D}_{12} \\ \mathbf{D}_{21} &\mathbf{D}_{22}
\end{pmatrix}
=\mathbf{I}_p
$$
gives the relationships
\begin{align*}
\mathbf{C}_{11}\mathbf{D}_{11}+\mathbf{C}_{12}\mathbf{D}_{21} &= \mathbf{I}_{p_1}\\
\mathbf{C}_{11}\mathbf{D}_{12}+\mathbf{C}_{12}\mathbf{D}_{22} &= \mathbf{O}_{p_1, p_2}\\
\mathbf{C}_{22}\mathbf{D}_{21}+\mathbf{C}_{21}\mathbf{D}_{11} &= \mathbf{O}_{p_2, p_1}\\
\mathbf{C}_{22}\mathbf{D}_{22}+\mathbf{C}_{21}\mathbf{D}_{12} &= \mathbf{I}_{p_2}\\
\end{align*}
from which we deduce that the so-called Schur complement of $\mathbf{C}_{22}$ is $$\mathbf{C}_{11}+\mathbf{C}_{12}\mathbf{C}^{-1}_{22}\mathbf{C}_{21} = \mathbf{D}_{11}^{-1}$$
and 
$$
-\mathbf{C}_{22}\mathbf{C}_{21}(\mathbf{C}_{11}+\mathbf{C}_{12}\mathbf{C}^{-1}_{22}\mathbf{C}_{21})^{-1} = \mathbf{D}_{21}.
$$
Substituting 
$$
\begin{pmatrix} \mathbf{C}_{11} & \mathbf{C}_{12} \\ \mathbf{C}_{21} &\mathbf{C}_{22}
\end{pmatrix} \equiv \begin{pmatrix} \mathbf{X}_1^\top\mathbf{X}_1 & \mathbf{X}_1^\top\mathbf{X}_2\\\mathbf{X}_2^\top\mathbf{X}_1  &\mathbf{X}_2^\top\mathbf{X}_2 
\end{pmatrix}
$$
and plug-in this result back in the equation for the least squares yields
\begin{align*}
\hat{\boldsymbol{\beta}}_1 &= (\mathbf{D}_{11}\mathbf{X}_1^\top + \mathbf{D}_{12}\mathbf{X}_2^\top)\boldsymbol{y} 
\\&= \mathbf{D}_{11}( \mathbf{X}_1^\top - \mathbf{C}_{12}\mathbf{C}_{22}^{-1}\mathbf{X}_2)\boldsymbol{y}
\\&= \left(\mathbf{C}_{11}+\mathbf{C}_{12}\mathbf{C}^{-1}_{22}\mathbf{C}_{21}\right)^{-1} \mathbf{X}_1^\top\mathbf{M}_{\mathbf{X}_2}\boldsymbol{y} 
\\&= (\mathbf{X}_1^\top\mathbf{M}_{\mathbf{X}_2}\mathbf{X}_1)^{-1}\mathbf{X}_1^\top\mathbf{M}_{\mathbf{X}_2}\boldsymbol{y}.
\end{align*}

The proof that the residuals are the same is left as an exercise.
```


The Frisch--Waugh--Lovell theorem dates back to the work of [Frisch, R. and F. Waugh (1933)](https://www.jstor.org/stable/1907330) and of [M. Lovell (1963)](https://doi.org/10.1080/01621459.1963.10480682).


## Revisiting the interpretation of the parameters of a linear model 

Geometrically, the linear model $\boldsymbol{y} = \mathbf{X} \boldsymbol{\beta} + \text{residuals}$ corresponds to the projection on to the span of $\mathbf{X}$ and gives the line of best fit in that space.


It is perhaps easiest to visualize the two-dimensional case, when $\mathbf{X} = (\mathbf{1}_n^\top, \mathbf{x}_1^\top)^\top$ is a $n \times 2$ design matrix and $\mathbf{x}_1$ is a continuous covariate. In this case, the coefficient vector $\boldsymbol{\beta}=(\beta_0, \beta_1)^\top$  represent, respectively, the intercept and the slope. 


If $\mathbf{X} = \mathbf{1}_n$, the model only consists of an intercept, which is interpreted as the mean level. Indeed, the projection matrix corresponding to $\mathbf{1}_n$, $\mathbf{H}_{\mathbf{1}_n}$, is a matrix whose entries are all identically $n^{-1}$. The fitted values of this model thus correspond to the mean of $\boldsymbol{y}$, $\bar{y}$ and the residuals are the centred values $\boldsymbol{y}-\mathbf{1}_n \bar{y}$ whose mean is zero.


More generally, for $\mathbf{X}$ an $n \times p$ design matrix, the interpretation is as follows: a unit increase in $\mathrm{x}_{ij}$ ($\mathrm{x}_{ij} \mapsto \mathrm{x}_{ij}+1)$ leads to a change of $\beta_j$ unit for $y_i$ ($y_i \mapsto \beta_j+y_i$), other things being held constant. Beware of models with higher order polynomials and interactions: if for example one is interested in the coefficient for $\mathbf{x}_j$, but $\mathbf{x}_j^2$ is also a column of the design matrix, then a change of one unit in $\mathbf{x}_j$ will not lead to a change of $\beta_jx_j$ for $y_j$!
  
The FWL theorem says the coefficient $\boldsymbol{\beta}_2$ in the regression 
\[\boldsymbol{y} =\mathbf{X}_1\boldsymbol{\beta}_1 + \mathbf{X}_2\boldsymbol{\beta}_2 + \boldsymbol{\varepsilon}\] is equivalent to that of the regression 
\[\mathbf{M}_1\boldsymbol{y} =\mathbf{M}_1 \mathbf{X}_2\boldsymbol{\beta}_2 + \boldsymbol{\varepsilon} \]
This can be useful to distangle the effect of one variable.

The intercept coefficient does not correspond to the mean of $\boldsymbol{y}$ unless the other variables in the design matrix have been centered (meaning they have mean zero). Otherwise, the coefficient $\beta_0$ associated to the intercept is nothing but the level of $y$ when all the other variables are set to zero.
Adding new variables affects the estimates of the coefficient vector $\boldsymbol{\beta}$, unless the new variables are orthogonal to the existing lot.


## Factors

Oftentimes, the regressors that are part of the design matrix will be categorical variables, which can be ordered or unordered. In the design matrix, these will appear as matrix of binary indicators. In **R**, dummy variables (vectors of class `factor`) are used to indicate categorical variables, which are often encoded as strings or (perhaps worse) integers. The function `read.table` has an argument, `stringsAsFactors`, that will automatically cast strings to factors. Oftentimes, dummy indicators are encoded as integer vectors in data frames. There is a risk that the vector be interpreted as a continuous numeric if the levels are integers (for example, advancement of the state of an illness that is encoded as 1, 2, 3, \ldots). 

Useful functions to deal with factors include 

- `as.factor`: casts column vectors to factors;  
- `is.factor`: to check whether the vector is a factor;
- `class`: reports the encoding (`factor` for factor objects);
- `summary`: displays counts of the various levels of a factor in place of the usual summary statistics;
- `levels`: the names of the different levels of a factor; can be used to replace existing category names by more meaningful ones;


The function `lm` knows how to deal with `factor` objects and will automatically transform it to a matrix of binary indiactors. For identifiability constraints, one level of the factor must be dropped and the convention in **R** is that the categories whose `levels` is first in alphabetical order is used as baseline. See `?factor` for more details.


Suppose your data set contains binary factors with only two levels and such that these are mutually exclusive. You may wish to merge these if they refer to the same variable, for example ethnicity. 
The following brute-force solution shows how to merge the factor vectors into a single factor. This will not change the model you get (since the design matrix would span the same space), but can affect the result of ANOVA calls since you will drop all different levels at once rather than subgroup by subgroup.

```{r mergefactors}
#Create dummy factors with different names to illustrate
a <- rbinom(500, size = 1, prob = 0.2)
b <- rep(0, 500)
b[a == 0] <- rbinom(sum(a == 0), size = 1, prob = 0.1)
#This is the output you get if they are encoded using 0/1
#Usually, they are columns of a data frame
newfactor <- data.frame(a = factor(a, labels = c("Hispanic","Other")), 
                        b = factor(b, labels = c("Black", "Not black")))
#Make them have different levels (important that the Other class be encoded as zero
newlev <- cbind(as.numeric(newfactor$a) - 1, as.numeric(newfactor$b) - 1) %*% c(1,2)
mergfactor <- factor(newlev, levels = c("1","2","0"), labels = c("Hispanic", "Black", "Other"))
```


Ordered factors do not have the same parametrization as unordered ones, so be careful when interpreting them.

## Example: seasonal effects

Suppose we are interested in estimating the seasonal model for quarterly data. Since each observation is recorded on one trimester, it could be postulated that the time of the year affect the response. This effect can be built in the design and tested for.

Consider a design matrix whose columns include $(\mathbf{S}_1, \mathbf{S}_2, \mathbf{S}_3, \mathbf{S}_4)$. The entries of the seasonal dummies are 0/1 depending on the season; for example, $\mathrm{S}_{i1}=1$ if the observation is recorded in the spring and $\mathrm{S}_{i1}=0$ otherwise, so that, for time ordered response vectors starting in the spring, we can write $\mathbf{S}_1=(1,0,0,0,1,0,0,\ldots)^\top$ and $\mathbf{S}_1 + \mathbf{S}_2 + \mathbf{S}_3 + \mathbf{S}_4 = \mathbf{1}_n$. Thus, a regression model cannot include a mean and all four seasonal variables, but since $\Sp(\mathbf{S}_1, \mathbf{S}_2, \mathbf{S}_3, \mathbf{S}_4) = \Sp(\mathbf{S}_1, \mathbf{S}_2, \mathbf{S}_3, \mathbf{1}_n) = \Sp(\mathbf{S}_2, \mathbf{S}_3, \mathbf{S}_4, \mathbf{1}_n)$, any set of four vectors can be used.

If we drop the constant vector $\mathbf{1}_n$ from the regression (in **R**, by writing `0` or `-1` on the right hand side of the `lm` formula), the coefficients $\alpha_i$, $i=1, \ldots, 4$ in regression 
$$\boldsymbol{y} = \mathbf{S}_1\alpha_1 +\mathbf{S}_2\alpha_2  + \mathbf{S}_3\alpha_3 + \mathbf{S}_4\alpha_4 + \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}$$
correspond to the intercept for season $i$. If we include instead the dummy $\mathbf{S}_1$ and fit
$$\boldsymbol{y} = \mathbf{1}_n\gamma_1 +\mathbf{S}_2\gamma_2  + \mathbf{S}_3\gamma_3 + \mathbf{S}_4\gamma_4 + \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon},$$
then the constant for season 1 is $\alpha_1=\gamma_1$, and is $\alpha_j=\gamma_1+\gamma_j$ for season $j \in \{2, 3, 4\}$. The reference level is therefore the first season $\gamma_1$ and the coefficients $\{\gamma_j\}, j \in \{2, 3, 4\}$ are contrasts (the difference between the baseline and the seasonal level).


As just illustrated above, the interpretation of the coefficients when we have factors differs. Introducing factors lead to different intercepts for the different groups, whereas interactions with the factors lead to different slopes. Since we cannot include all levels of the factor as well as an intercept, the interpretation of the coefficient for the constant is the intercept of the baseline, i.e., the ommitted factor. 

For simplicity, consider the simple case with an intercept and a single factor. Why do the coefficients $\beta_1$ and $\beta_1'$ associated to the first columns of models
\begin{align*}
 \mathbf{X}\boldsymbol{\beta} = \begin{pmatrix}
       \mathbf{1}_{n_1} & \mathbf{0}_{n_1} \\
       \mathbf{1}_{n_2} & \mathbf{1}_{n_2}  \\
      \end{pmatrix}\begin{pmatrix} \beta_1 & \beta_2\end{pmatrix}^\top, \qquad 
      \mathbf{X}'\boldsymbol{\beta}' =  \begin{pmatrix}
       \mathbf{1}_{n_1} & \mathbf{0}_{n_1} \\
       \mathbf{0}_{n_2} & \mathbf{1}_{n_2}  \\
      \end{pmatrix}\begin{pmatrix} \beta_1' & \beta_2'\end{pmatrix}^\top
\end{align*}
have the same interpretation? For the matrix $\mathbf{X}$ consisting of orthogonal regressors, this is 
clear. For the matrix $\mathbf{X}'$, recall the FWL theorem: the regression coefficient $\beta_1'$ is the same as that of the regression 
$\Mmat_{\mathbf{X}_2}\bs{y} = \Mmat_{\mathbf{X}_2}\mathbf{1}_n + \bs{u}$ for $\mathbf{X}_2 = (\mathbf{0}_{n_1}^\top, \mathbf{1}_{n_2}^\top)^\top$. The matrix 
$\Mmat_{\mathbf{X}_2}$ is a block matrix, whose first $n_1 \times n_1$ block contains entries $n_1^{-1}$ and the rest of the entries is zero.
$\Mmat_{\mathbf{X}_2}\bs{y}$ does not affect the last $n_2$ entries of $\bs{y}$, while $\Mmat_{\mathbf{X}_2}\mathbf{1}_n = \mathbf{1}_n - \mathbf{X}_2$. This 
reasoning generalizes to more complex settings with a slope and other regressors.



The discussion is illustrated on a dataset consisting of quarterly measurements of the gas consumption in the United Kingdom, from 1960 to 1986.

```{r}
data(UKgas)
quarter <- rep(1:4, length = length(UKgas)) #create vector 1, 2, 3, 4, 1, ...
is.factor(quarter)
quarter <- as.factor(quarter) #cast the vector to a factor
is.factor(quarter)
class(quarter)
#What is name of the dummies
head(quarter)
levels(quarter)
levels(quarter) <- c("Q1", "Q2", "Q3", "Q4") #change names
```

The first model is of the form 
$$ \boldsymbol{y} = \mathbf{Q}_1 \alpha_1 + \mathbf{Q}_2 \alpha_2 + \mathbf{Q}_3 \alpha_3 + \mathbf{Q}_4 \alpha_4 + \boldsymbol{\varepsilon}.
$$

```{r}
gas_lm1 <- lm(log(UKgas) ~ quarter - 1)
coef(gas_lm1)
head(model.matrix(gas_lm1), 10)
```

The model with all the quarterly dummies gives the quarterly average value $\exp(\alpha_j)$ in quarter $j$, $j = 1, \ldots, 4$. 
If we include an intercept, the first factor is selected as baseline and the coefficients of the quarters `Q2` to `Q4` are contrasts.
For this model, say
$$ \boldsymbol{y} = \mathbf{1}_n \gamma_1 + \mathbf{Q}_2 \gamma_2 + \mathbf{Q}_3 \gamma_3 + \mathbf{Q}_4 \gamma_4 + \boldsymbol{\varepsilon},
$$

```{r}
#New parameterization, with a constant
gas_lm2 <- lm(log(UKgas) ~ quarter) #R drops collinear by default and fits with Q2-Q4
coef(gas_lm2)
head(model.matrix(gas_lm2), 10)
```

The estimated average gas consumption in millions of therms is $\exp(\widehat{\gamma}_1)=\exp(\widehat{\alpha}_1)$ for the first quarter, $\exp(\widehat{\alpha}_2)=\exp(\widehat{\gamma}_1+\widehat{\gamma}_2)$ for the second quarter, etc. 

We can check that the interpretation is correct.
```{r}
isTRUE(all.equal((coef(gas_lm2)[1] + coef(gas_lm2)[-1]), 
                 coef(gas_lm1)[-1], check.attributes = FALSE))
```

## Solutions 

### Exercise 4.4

Consider the linear model $$\boldsymbol{y} = \mathbf{X}_1\boldsymbol{\beta}_1 +\mathbf{X}_2\boldsymbol{\beta}_1 + \boldsymbol{\varepsilon}$$ and suppose that $\mathbf{X}_1$ includes a column of ones.


```{r}
data(Auto, package = "ISLR")
y <- Auto$mpg
X1 <- cbind(1, Auto$horsepower)
X2 <- Auto$weight
```

a. Form the projection matrices $\mathbf{H}_{\mathbf{X}}$, $\mathbf{H}_{1}$, $\mathbf{H}_{2}$ and the complementary projection matrices (the functions `cbind`, `%*%`, `solve` and `t` may be useful).

```{r sol3.4, results="hide"}
data(Auto, package = "ISLR")
y <- Auto$mpg
X1 <- cbind(1, Auto$horsepower)
X2 <- Auto$weight
##Warning: output not displayed 
#Projection matrices
X <- cbind(X1, X2) 
#Helper functions
#Create a function: function(args){ ...}
#last line will be object returned (if not assigned)
#otherwise use `return( )`: see below for example
proj_mat <- function(x){
  x %*% solve(t(x) %*% x) %*% t(x)
}
coefs_vals <- function(x, y){
  coefs <- c(solve(t(x) %*% x) %*% t(x) %*% y) 
  return(coefs)
  #`c` transform output from n x 1 matrix to n-vector
}
resid_vals <- function(y, pmat){
  c((diag(1, length(y)) - pmat)  %*% y) 
  #diag(1, ...) creates identity matrix
}
fitted_vals <- function(y, pmat){
  c(pmat %*% y)
}

#Create projection matrices
H  <- proj_mat(X)
H1 <- proj_mat(X1)
H2 <- proj_mat(X2)
M  <- diag(nrow(X)) - H
M1 <- diag(nrow(X)) - H1
M2 <- diag(nrow(X)) - H2
```

b. Obtain the OLS estimates $\widehat{\boldsymbol{\beta}}_1$, $\widehat{\boldsymbol{\beta}}_2$
c. Use the projection matrices to obtain the fitted values $\widehat{\boldsymbol{y}}$ and the estimated residuals $\widehat{\boldsymbol{\varepsilon}}$.

```{r sol3.4b, results = "hide"}
#OLS coefficients
beta_hat <- coefs_vals(X, y)
beta_hat
#Fitted values
fitted_vals(y, H)
#Residuals
resid_vals(y, H)
```

d. What happens to the residuals if your regressors do not include a vector of constants?

 If a constant is not included, the residuals are not centered unless the columns of the design matrix and the response were
centered, meaning they had expectation zero. This is why a column vector of ones is always included unless the mean is known
(from theory or otherwise) to be zero.

```{r sol3.4c}
#Removing the row of constants
res_uncentered <- resid_vals(y, proj_mat(X[,-1])) #subset [row, column]
#X[,-1] take all rows, all columns but first
mean(res_uncentered)
#Consequence of not centering is that residuals are not mean zero
```


e. Verify numerically Frisch--Waugh--Lovell's theorem and test the different regression models from Exercice 2.4 to validate your answers.

```{r sol3.4d}
#The null models regression has
coef_0 <- coefs_vals(x = X, y = y)[ncol(X)]
res_0  <- resid_vals (y = y, pmat = H)
#The following function checks equality of the coefficients
#beta 2 is the last coef (here 1d)
check_FWL <- function(xmat, yvec, coef0 = coef_0, res0 = res_0){
  c(isTRUE(all.equal(coefs_vals(x = xmat, y = yvec)[ncol(xmat)], coef0)),
    isTRUE(all.equal(resid_vals(y = yvec, pmat = proj_mat(xmat)), res0))
  )
}

#Check the results of 4.3
res_mat <- cbind(check_FWL(yvec = y, xmat = X2), #1
check_FWL(yvec = H1 %*% y, xmat = X2), #2
check_FWL(yvec = H1 %*% y, xmat = H1 %*% X2), #3
check_FWL(yvec = H %*% y, xmat = X), #4
check_FWL(yvec = H %*% y, xmat = X2), #5
check_FWL(yvec = M1 %*% y, xmat = X2), #6
check_FWL(yvec = M1 %*% y, xmat = M1 %*% X2), #7 and 9
check_FWL(yvec = M1 %*% y, xmat = cbind(X1, M1 %*% X2)), #8
check_FWL(yvec = H %*% y, xmat = M1 %*% X2)) #10
colnames(res_mat) <-  c("(1)", "(2)", "(3)", "(4)", "(5)", "(6)", "(7,9)","(8)","(10)")
rownames(res_mat) <- c("coefficients","residuals")
print(res_mat)
```

# Frisch--Waugh--Lovell theorem

\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\Hmat}{\mathbf{H}}
\newcommand{\Mmat}{\mathbf{M}}
\newcommand{\mX}{\mathbf{X}}
\newcommand{\bX}{{\mathbf{X}}}
\newcommand{\bx}{{\mathbf{x}}}
\newcommand{\by}{{\boldsymbol{y}}}
\newcommand{\bY}{{\boldsymbol{Y}}}
\newcommand{\eps}{\varepsilon}
\newcommand{\beps}{\boldsymbol{\varepsilon}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\hbb}{\widehat{\boldsymbol{\beta}}}
\newcommand{\limni}{\lim_{n \ra \infty}}
\newcommand{\Sp}{\mathsf{span}}

This result dates back to the work of [Frisch, R. and F. Waugh (1933)](https://www.jstor.org/stable/1907330) and of [M. Lovell (1963)](https://doi.org/10.1080/01621459.1963.10480682). The FWL theorem has two components: it gives a formula for partitioned OLS estimates and shows that residuals from sequential regressions are identical.

Consider the following linear regression 
$$
 \by = \bX_1\bbeta_1+\bX_2\bbeta_2+ \bs{u}, \label{eq1}
$$
where the response vector $\by$ is $n \times 1$, the vector of errors $\bs{u}$ is a realization from a mean zero random 
variable. The $n \times p$ full-rank design matrix $\bX$ can be written as the partitioned 
matrix $(\bX_1^\top, \bX_2^\top)^\top$ with blocks $\bX_1$, an $n \times p_1$ matrix, and $\bX_2$, an $n \times p_2$ matrix. Let 
$\hbb_1$ 
and $\hbb_2$ be the ordinary 
least square (OLS) parameter estimates from running this regression. Define the orthogonal projection matrix $\Hmat_\bX$ as usual and 
$\Hmat_{\bX_i} = 
\bX_i^{\vphantom{\top}}(\bX_i^\top\bX_i^{\vphantom{\top}})^{-1}\bX_i^\top$ for $i=1, 2$. Similarly, 
define the complementary projection matrices $\Mmat_{\bX_1}=\mathbf{I}_n-\Hmat_{\bX_1}$ and $\Mmat_{\bX_2}=\mathbf{I}_n-\Hmat_{\bX_2}$.

```{theorem}
The ordinary least square estimates of $\bbeta_2$ and the residuals from \eqref{eq1} are identical to those obtained by 
running the regression 
$$
 \Mmat_{\bX_1}\by = \Mmat_{\bX_1}\bX_2\bbeta_2 + \text{residuals}. \label{eq2} \
$$
```

```{proof}
The easiest proof uses projection matrices, but we demonstrate the result for OLS coefficients directly.
Consider an invertible $d \times d$ matrix $\mathbf{C}$ and denote its inverse by $\mathbf{D}$; then
$$
\begin{pmatrix} \mathbf{C}_{11} & \mathbf{C}_{12} \\ \mathbf{C}_{21} &\mathbf{C}_{22}
\end{pmatrix}\begin{pmatrix} \mathbf{D}_{11} & \mathbf{D}_{12} \\ \mathbf{D}_{21} &\mathbf{D}_{22}
\end{pmatrix}
=\mathbf{I}_p
$$
gives the relationships
\begin{align*}
\mathbf{C}_{11}\mathbf{D}_{11}+\mathbf{C}_{12}\mathbf{D}_{21} &= \mathbf{I}_{p_1}\\
\mathbf{C}_{11}\mathbf{D}_{12}+\mathbf{C}_{12}\mathbf{D}_{22} &= \mathbf{O}_{p_1, p_2}\\
\mathbf{C}_{22}\mathbf{D}_{21}+\mathbf{C}_{21}\mathbf{D}_{11} &= \mathbf{O}_{p_2, p_1}\\
\mathbf{C}_{22}\mathbf{D}_{22}+\mathbf{C}_{21}\mathbf{D}_{12} &= \mathbf{I}_{p_2}\\
\end{align*}
from which we deduce that the so-called Schur complement of $\mathbf{C}_{22}$ is $$\mathbf{C}_{11}+\mathbf{C}_{12}\mathbf{C}^{-1}_{22}\mathbf{C}_{21} = \mathbf{D}_{11}^{-1}$$
and 
$$
-\mathbf{C}_{22}\mathbf{C}_{21}(\mathbf{C}_{11}+\mathbf{C}_{12}\mathbf{C}^{-1}_{22}\mathbf{C}_{21})^{-1} = \mathbf{D}_{21}.
$$
Substituting 
$$
\begin{pmatrix} \mathbf{C}_{11} & \mathbf{C}_{12} \\ \mathbf{C}_{21} &\mathbf{C}_{22}
\end{pmatrix} \equiv \begin{pmatrix} \mathbf{X}_1^\top\mathbf{X}_1 & \mathbf{X}_1^\top\mathbf{X}_2\\\mathbf{X}_2^\top\mathbf{X}_1  &\mathbf{X}_2^\top\mathbf{X}_2 
\end{pmatrix}
$$
and plug-in this result back in the equation for the least squares yields
\begin{align*}
\hat{\boldsymbol{\beta}}_1 &= (\mathbf{D}_{11}\mathbf{X}_1^\top + \mathbf{D}_{12}\mathbf{X}_2^\top)\boldsymbol{y} 
\\&= \mathbf{D}_{11}( \mathbf{X}_1^\top - \mathbf{C}_{12}\mathbf{C}_{22}^{-1}\mathbf{X}_2)\boldsymbol{y}
\\&= \left(\mathbf{C}_{11}+\mathbf{C}_{12}\mathbf{C}^{-1}_{22}\mathbf{C}_{21}\right)^{-1} \mathbf{X}_1^\top\mathbf{M}_{\mathbf{X}_2}\boldsymbol{y} 
\\&= (\mathbf{X}_1^\top\mathbf{M}_{\mathbf{X}_2}\mathbf{X}_1)^{-1}\mathbf{X}_1^\top\mathbf{M}_{\mathbf{X}_2}\boldsymbol{y}.
\end{align*}

The proof that the residuals are the same is left as an exercise.
```

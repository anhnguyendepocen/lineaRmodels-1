[
["index.html", "lineaRmodels Preliminary remarks", " lineaRmodels Léo Belzile version of 2018-09-23 Preliminary remarks This is a web complement to MATH 341 (Linear Models), a first regression course for EPFL mathematicians. We shall use the R programming language througout the course (as it is free and it is used in other statistics courses at EPFL). Visit the R-project website to download the program. The most popular graphical cross-platform front-end is RStudio Desktop. R is an object-oriented interpreted language. It differs from usual programming languages in that it is designed for interactive analyses. Since R is not a conventional programming language, my teaching approach will be learning-by-doing. The benefit of using Rmarkdown is that you see the output directly and you can also copy the code. "],
["introduction.html", "1 Introduction", " 1 Introduction You can find several introductions to R online. Have a look at the R manuals or better at contributed manuals. A nice official reference is An introduction to R. You may wish to look up the following chapters of the R language definition (Evaluation of expressions and part of the Objects chapter). "],
["basics-of-r.html", "1.1 Basics of R", " 1.1 Basics of R 1.1.1 Help Help can be accessed via help or simply ?. If you do not know what to query, use ?? in front of a string, delimited by captions &quot; &quot; as in ??&quot;Cholesky decomposition&quot;. Help is your best friend if you don’t know what a function does, what are its arguments, etc. 1.1.2 Basic commands Basic R commands are fairly intuitive, especially if you want to use R as a calculator. Elementary functions such as sum, min, max, sqrt, log, exp, etc., are self-explanatory. Some unconventional features of the language: Use &lt;- to assign to a variable, and = for matching arguments inside functions Indexing in R starts at 1, not zero. Most functions in R are vectorized, so avoid loops as much as possible. Integers are obtained by appending L to the number, so 2L is an integer and 2 a double. Besides integers and doubles, the common types are - logicals (TRUE and FALSE); - null pointers (NULL), which can be assigned to arguments; - missing values, namely NA or NaN. These can also be obtained a result of invalid mathematical operations such as log(-2). The above illustrates a caveat of R: invalid calls will often returns something rather than an error. It is therefore good practice to check that the output is sensical. 1.1.3 Linear algebra in R R is an object oriented language, and the basic elements in R are (column) vector. Below is a glossary with some useful commands for performing basic manipulation of vectors and matrix operations: c creates a vector cbind (rbind) binds column (row) vectors matrix and vector are constructors diag creates a diagonal matrix (by default with ones) t is the function for transpose solve performs matrix inversion %*% is matrix multiplication, * is element-wise multiplication crossprod(A, B) calculates the cross-product \\(\\mathbf{A}^\\top\\mathbf{B}\\), t(A) %*% B, of two matrices A and B. eigen/chol/qr/svd perform respectively an eigendecomposition/Cholesky/QR/singular value decomposition of a matrix rep creates a vector of duplicates, seq a sequence. For integers \\(i\\), \\(j\\) with \\(i&lt;j\\), i:j generates the sequence \\(i, i+1, \\ldots, j-1, j\\). Subsetting is fairly intuitive and general; you can use vectors, logical statements. For example, if x is a vector, then x[2] returns the second element x[-2] returns all but the second element x[1:5] returns the first five elements x[(length(x) - 5):length(x)] returns the last five elements x[c(1, 2, 4)] returns the first, second and fourth element x[x &gt; 3] return any element greater than 3. Possibly an empty vector of length zero! x[ x &lt; -2 | x &gt; 2] multiple logical conditions. which(x == max(x)) index of elements satisfying a logical condition. For a matrix x, subsetting now involves dimensions: [1,2] returns the element in the first row, second column. x[,2] will return all of the rows, but only the second column. For lists, you can use [[ for subsetting by index or the $ sign by names. 1.1.4 Packages The great strength of R comes from its contributed libraries (called packages), which contain functions and datasets provided by third parties. Some of these (base, stats, graphics, etc.) are loaded by default whenever you open a session. To install a package from CRAN, use install.packages(&quot;package&quot;), replacing package by the package name. Once installed, packages can be loaded using library(package); all the functions in package will be available in the environment. There are drawbacks to loading packages: if an object with the same name from another package is already present in your environment, it will be hidden. Use the double-colon operator :: to access a single object from an installed package (package::object). "],
["week1.html", "1.2 Tutorial 1", " 1.2 Tutorial 1 1.2.1 Data sets Data sets are typically stored inside a data.frame, a matrix-like object whose columns contain the variables and the rows the observation vectors. The columns can be of different types (integer, double, logical, character), but all the column vectors must be of the same length. Variable names can be displayed by using names(faithful). Individual columns can be accessed using the column name using the $ operator. For example, faithful$eruptions will return the first column of the faithful dataset. To load a data set from an (installed) R package, use the command data with the name of the package as an argument (must be a string). The package datasets is loaded by default whenever you open R, so these are always in the search path. The following functions can be useful to get a quick glimpse of the data: summary provides descriptive statistics for the variable. str provides the first few elements with each variable, along with the dimension head (tail) prints the first (last) \\(n\\) lines of the object to the console (default is \\(n=6\\)). We start by loading a dataset of the Old Faithful Geyser of Yellowstone National park and looking at its entries. # Load Old faithful dataset data(faithful, package = &quot;datasets&quot;) # Query the database for documentation ?faithful # look at first entries head(faithful) ## eruptions waiting ## 1 3.600 79 ## 2 1.800 54 ## 3 3.333 74 ## 4 2.283 62 ## 5 4.533 85 ## 6 2.883 55 str(faithful) ## &#39;data.frame&#39;: 272 obs. of 2 variables: ## $ eruptions: num 3.6 1.8 3.33 2.28 4.53 ... ## $ waiting : num 79 54 74 62 85 55 88 85 51 85 ... # What kind of object is faithful? class(faithful) ## [1] &quot;data.frame&quot; Other common classes of objects: - matrix: an object with attributes dim, ncol and nrow in addition to length, which gives the total number of elements. - array: a higher dimensional extension of matrix with arguments dim and dimnames. - list: an unstructured class whose elements are accessed using double indexing [[ ]] and elements are typically accessed using $ symbol with names. To delete an element from a list, assign NULL to it. - data.frame is a special type of list where all the elements are vectors of potentially different type, but of the same length. 1.2.2 Graphics The faithful dataset consists of two variables: the regressand waiting and the regressor eruptions. One could postulate that the waiting time between eruptions will be smaller if the eruption time is small, since pressure needs to build up for the eruption to happen. We can look at the data to see if there is a linear relationship between the variables. An image is worth a thousand words and in statistics, visualization is crucial. Scatterplots are produced using the function plot. You can control the graphic console options using par — see ?plot and ?par for a description of the basic and advanced options available. Once plot has been called, you can add additional observations as points (lines) to the graph using point (lines) in place of plot. If you want to add a line (horizontal, vertical, or with known intercept and slope), use the function abline. Other functions worth mentioning at this stage: boxplot creates a box-and-whiskers plot hist creates an histogram, either on frequency or probability scale (option freq = FALSE). breaks control the number of bins. rug adds lines below the graph indicating the value of the observations. pairs creates a matrix of scatterplots, akin to plot for data frame objects. There are two options for basic graphics: the base graphics package and the package ggplot2. The latter is a more recent proposal that builds on a modular approach and is more easily customizable — I suggest you stick to either and ggplot2 is a good option if you don’t know R already, as the learning curve will be about the same. Even if the display from ggplot2 is nicer, this is no excuse for not making proper graphics. Always label the axis and include measurement units! # Scatterplots # Using default R commands plot(waiting ~ eruptions, data = faithful, xlab = &quot;Eruption time (in min.)&quot;, ylab = &quot;Waiting time between eruptions (in min.)&quot;, main = &quot;Old Faithful Geyser Data&quot;) #using the grammar of graphics (more modular) #install.packages(&quot;ggplot2&quot;) #do this once only library(ggplot2) ggplot2::ggplot(data = faithful, aes(x = eruptions, y = waiting)) + geom_point() + labs(title = &quot;Old Faithful Geyser Data&quot;, x = &quot;Eruption time (in min.)&quot;, y = &quot;Waiting time between eruptions (in min.)&quot;) A simple linear model of the form \\[y_i = \\beta_0 + \\beta_1 \\mathrm{x}_i + \\varepsilon_i,\\] where \\(\\varepsilon_i\\) is a noise variable with expectation zero and \\(\\mathbf{x} = \\mathsf{eruptions}\\) and \\(\\boldsymbol{y} = \\mathsf{waiting}\\). We first create a matrix with a column of \\(\\mathbf{1}_n\\) for the intercept. We bind vectors by column (cbind) into a matrix, recycling arguments if necessary. Use $ to obtain a column of the data frame based on the name of the variable (partial matching is allowed, e.g., faithful$er is equivalent to faithful$eruptions in this case). ## Manipulating matrices n &lt;- nrow(faithful) p &lt;- ncol(faithful) y &lt;- faithful$waiting X &lt;- cbind(1, faithful$eruptions) 1.2.3 Projection matrices Recall that \\(\\mathbf{H}_{\\mathbf{X}} \\equiv \\mathbf{X}(\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\) is the orthogonal projection matrix onto \\(\\mathsf{span}(\\mathbf{X})\\). The latter has \\(p=2\\) eigenvalues equal to 1, is an \\(n \\times n\\) matrix of rank \\(p\\), is symmetric and idempotent. We can verify the properties of \\(\\mathbf{H}_{\\mathbf{X}}\\) numerically. Whereas we will frequently use == to check for equality of booleans, the latter should be avoided for comparisons because computer arithmetic is exact only in base 2. For example, 1/10 + 2/10 - 3/10 == 0 will return FALSE, whereas all.equal(1/10 + 2/10 - 3/10, 0) will return TRUE. Use all.equal to check for equalities. Hx &lt;- X %*% solve(crossprod(X)) %*% t(X) # Create projection matrix onto complement # `diag(n)` is the n by n identity matrix Mx &lt;- diag(n) - Hx #Check that projection leaves X invariant isTRUE(all.equal(X, Hx %*% X)) ## [1] TRUE #Check that orthogonal projection maps X to zero matrix of dimension (n, p) isTRUE(all.equal(matrix(0, nrow = n, ncol = p), Mx %*% X)) ## [1] TRUE #Check that the matrix Hx is idempotent isTRUE(all.equal(Hx %*% Hx, Hx)) ## [1] TRUE #Check that the matrix Hx is symmetric isTRUE(all.equal(t(Hx), Hx)) ## [1] TRUE #Check that only a two eigenvalue are 1 and the rest are zero isTRUE(all.equal(eigen(Hx, only.values = TRUE)$values, c(rep(1, p), rep(0, n - p)))) ## [1] TRUE #Check that the matrix has rank p isTRUE(all.equal(Matrix::rankMatrix(Hx), p, check.attributes = FALSE)) ## [1] TRUE 1.2.4 Your turn Install the R package ISLR and load the dataset Auto. Be careful, as R is case-sensitive. Query the help file for information about the data set. Look at the first lines of Auto Create an explanatory variable x with horsepower and mileage per gallon as response y. Create a scatterplot of y against x. Is there a linear relationship between the two variables? Append a vector of ones to x and create a projection matrix. Check that the resulting projection matrix is symmetric and idempotent. "],
["week-1-exercises.html", "1.3 Week 1 exercises", " 1.3 Week 1 exercises (1.4) Oblique projections Suppose that \\(\\mathsf{span}(\\mathbf{X}) \\neq \\mathsf{span}(\\mathbf{W})\\), that both \\(\\mathbf{X}\\) and \\(\\mathbf{W}\\) are full-rank \\(n \\times p\\) matrices such that \\(\\mathbf{X}^\\top\\mathbf{W}\\) and \\(\\mathbf{W}^\\top\\mathbf{X}\\) are invertible. An oblique projection matrix is of the form \\(\\mathbf{P}\\equiv\\mathbf{X}(\\mathbf{W}^\\top\\mathbf{X})^{-1}\\mathbf{W}^\\top\\) and appears in instrumental variable regression. The oblique projection is such that \\(\\mathrm{im}(\\mathbf{P})=\\mathsf{span}(\\mathbf{X})\\), but \\(\\mathrm{im}(\\mathbf{I}-\\mathbf{P})=\\mathsf{span}(\\mathbf{W}^\\perp)\\). This fact is illustrated below. We consider two non-parallel vectors in \\(\\mathbb{R}^2\\), \\(\\mathbf{X}\\) and \\(\\mathbf{W}\\). The figure shows the projection of a third vector (non-parallel to \\(\\mathbf{X}\\) or \\(\\mathbf{W}\\)) onto the span of \\(\\mathbf{P}\\) (blue), \\(\\mathbf{P}^\\top\\) (red), \\(\\mathbf{I}_2-\\mathbf{P}\\) (dashed blue) and \\(\\mathbf{I}_2-\\mathbf{P}^\\top\\) (dashed red). The circles indicate the points \\(\\mathbf{W}\\) (red) and \\(\\mathbf{X}\\) (blue) on the plane. Notice that \\(\\mathbf{I}_2-\\mathbf{P}^\\top \\perp \\mathbf{P}\\), whereas \\(\\mathbf{I}_2-\\mathbf{P} \\perp \\mathbf{P}^\\top\\). #Create two vectors (non-parallel) x &lt;- c(1, 2) w &lt;- c(-1, 0.1) #Create oblique projection matrix P &lt;- x %*% solve(t(w) %*% x) %*% t(w) isTRUE(all.equal((P %*% P), P)) #P is idempotent ## [1] TRUE P - t(P) #but not symmetric ## [,1] [,2] ## [1,] 0.000 -2.625 ## [2,] 2.625 0.000 #Project a third vector `vec&#39; onto P, P transpose, I-P, I-(P transpose) vec &lt;- c(1.9, -1.5) vec_P &lt;- P %*% vec vec_Pt &lt;- t(P) %*% vec vec_Id_minus_P &lt;- (diag(2)-P) %*% vec #diag: diagonal matrix, default to identity vec_Id_minus_Pt &lt;- (diag(2)-t(P)) %*% vec #Plot the resulting vector along with the two vectors x and w (points) par(pty = &quot;s&quot;) #graphical console parameters (square region) plot(NULL, xlab = &quot;x&quot;, ylab = &quot;y&quot;, xlim = c(-4, 4), ylim = c(-4, 4)) #create empty plot with labels x and y, on square region [-4,4]^2 points(0, 0, pch = 20) #points: add points to existing plot points(x[1], x[2], col = &quot;blue&quot;) points(w[1], w[2], col = &quot;red&quot;) # blue line for P, dashed blue for I-P, red for Pt, red dashed for I-Pt segments(x0 = 0, y0 = 0, x1 = vec_P[1], y1 = vec_P[2], col = &quot;blue&quot;) segments(x0 = 0, y0 = 0, x1 = vec_Pt[1], y1 = vec_Pt[2], col = &quot;red&quot;) segments(x0 = 0, y0 = 0, x1 = vec_Id_minus_P[1], y1 = vec_Id_minus_P[2], col = &quot;blue&quot;, lty = 2) segments(x0 = 0, y0 = 0, x1 = vec_Id_minus_Pt[1], y1 = vec_Id_minus_Pt[2], col = &quot;red&quot;, lty = 2) "],
["computational-considerations.html", "2 Computational considerations", " 2 Computational considerations In this tutorial, we will explore some basic R commands and illustrate their use on the Motor Trend Car Road Tests dataset (mtcars). "],
["calculation-of-least-square-estimates.html", "2.1 Calculation of least square estimates", " 2.1 Calculation of least square estimates Consider as usual \\(\\boldsymbol{y}\\) and \\(n\\)-vector of response variables and a full-rank \\(n \\times p\\) design matrix \\(\\mathbf{X}\\). We are interested in finding the ordinary least square coefficient \\(\\hat{\\boldsymbol{\\beta}}\\), the fitted values \\(\\hat{\\boldsymbol{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\) and the residuals \\(\\boldsymbol{e} = \\boldsymbol{y} - \\mathbf{X}\\boldsymbol{\\beta}\\). Whereas orthogonal projection matrices are useful for theoretical derivations, they are not used for computations. Building \\(\\mathbf{H}_{\\mathbf{X}}\\) involves a matrix inversion and the storage of an \\(n \\times n\\) matrix. In Exercise series 2, we looked at two matrix decompositions: a singular value decomposition (SVD) and a QR decomposition. These are more numerically stable than using the normal equations \\((\\mathbf{X}^\\top\\mathbf{X})\\boldsymbol{\\beta} = \\mathbf{X}^\\top\\boldsymbol{y}\\) (the condition number of the matrix \\(\\mathbf{X}^\\top\\mathbf{X}\\) is the square of that of \\(\\mathbf{X}\\) — more on this later). For more details about the complexity and algorithms underlying the different methods, the reader is referred to these notes of www.math.uchicago.edu/~may/REU2012/REUPapers/Lee.pdf. This material is optional. 2.1.1 Normal equations The following simply illustrates what has been derived in Exercise series 2. You will probably never use these commands, as R has devoted functions that are coded more efficiently. We can compute first the ordinary least square estimates using the formula \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\boldsymbol{y}\\). data(mtcars) y &lt;- mtcars$mpg X &lt;- cbind(1, as.matrix(mtcars[,2:ncol(mtcars)])) n &lt;- nrow(X) p &lt;- ncol(X) # Estimation of betahat: XtX&lt;- crossprod(X) Xty &lt;- crossprod(X, y) # Solve normal equations betahat &lt;- as.vector(solve(XtX, Xty)) #same as betahat &lt;- solve(t(X) %*% X) %*% t(X) %*% y 2.1.2 Singular value decomposition The SVD decomposition in R returns a list with elements u, d and v. u is the orthonormal \\(n \\times p\\) matrix, d is a vector containing the diagonal elements of \\(\\mathbf{D}\\) and v is the \\(p \\times p\\) orthogonal matrix. Recall that the decomposition is \\[\\mathbf{X} = \\mathbf{UDV}^\\top\\] and that \\(\\mathbf{VV}^\\top= \\mathbf{V}^\\top\\mathbf{V}=\\mathbf{U}^\\top\\mathbf{U}=\\mathbf{I}_p\\). The matrix \\(\\mathbf{D}\\) contains the singular values of \\(\\mathbf{X}\\), and the diagonal elements \\(\\mathrm{d}_{ii}^2\\) corresponds to the (ordered) eigenvalues of \\(\\mathbf{X}^\\top\\mathbf{X}\\). svdX &lt;- svd(X) # Projection matrix Hx &lt;- tcrossprod(svdX$u) # t(U) %*% U gives p by p identity matrix all.equal(crossprod(svdX$u), diag(p)) ## [1] TRUE # V is an orthogonal matrix all.equal(tcrossprod(svdX$v), diag(p)) ## [1] TRUE all.equal(crossprod(svdX$v), diag(p)) ## [1] TRUE # D contains singular values all.equal(svdX$d^2, eigen(XtX, only.values = TRUE)$values) ## [1] TRUE # OLS coefficient from SVD betahat_svd &lt;- c(svdX$v %*% diag(1/svdX$d) %*% t(svdX$u) %*% y) all.equal(betahat_svd, betahat) ## [1] TRUE 2.1.3 QR decomposition R uses a QR-decomposition to calculate the OLS. There are specific functions to return coefficients, fitted values and residuals. One can also obtain the \\(n \\times p\\) matrix \\(\\mathbf{Q}_1\\) and the upper triangular \\(p \\times p\\) matrix \\(\\mathbf{R}\\) from the thinned QR decomposition, \\[\\mathbf{X} = \\mathbf{Q}_1\\mathbf{R}.\\] qrX &lt;- qr(X) Q1 &lt;- qr.Q(qrX) R &lt;- qr.R(qrX) # Compute betahat from QR betahat_qr1 &lt;- qr.coef(qrX, y) #using built-in function betahat_qr2 &lt;- c(backsolve(R, t(Q1) %*% y)) #manually all.equal(betahat, betahat_qr1, check.attributes = FALSE) ## [1] TRUE all.equal(betahat, betahat_qr2, check.attributes = FALSE) ## [1] TRUE # Compute residuals qre &lt;- qr.resid(qrX, y) all.equal(qre, c(y - X %*% betahat), check.attributes = FALSE) ## [1] TRUE # Compute fitted values qryhat &lt;- qr.fitted(qrX, y) all.equal(qryhat, c(X %*% betahat), check.attributes = FALSE) ## [1] TRUE # Compute orthogonal projection matrix qrHx &lt;- tcrossprod(Q1) all.equal(qrHx, Hx) ## [1] TRUE "],
["parameter-estimation.html", "2.2 Parameter estimation", " 2.2 Parameter estimation We are now ready to fit a linear model with an intercept and a linear effect for the weight. The model will be of the form \\[ \\texttt{mpg}_i = \\beta_0 + \\texttt{wt}_i\\beta_1 +\\varepsilon_i.\\] We form the design matrix \\((\\boldsymbol{1}_n^\\top, \\texttt{wt}^\\top)^\\top\\) and the vector of regressand \\(\\texttt{mpg}\\), then proceed with calculating the OLS coefficients \\(\\hat{\\boldsymbol{\\beta}}\\), the hat matrix \\(\\mathbf{H}_{\\mathbf{X}}\\), the fitted values \\(\\hat{\\boldsymbol{y}}\\) and the residuals \\(\\boldsymbol{e}\\). #Design matrix wt &lt;- mtcars$wt X &lt;- cbind(1, wt) mpg &lt;- mtcars$mpg #OLS estimates XtXinv &lt;- solve(crossprod(X)) beta_hat &lt;- c(XtXinv %*% t(X) %*% mpg) #Form orthogonal projection matrix Hmat &lt;- X %*% XtXinv %*% t(X) #Create residuals and fitted values fitted &lt;- Hmat %*% mpg res &lt;- mpg - fitted fitted &lt;- Hmat %*% mpg #Variance estimate and standard errors s2 &lt;- sum(res^2) / (length(res) - ncol(X)) std_err &lt;- sqrt(diag(s2 * XtXinv)) The residuals \\(\\boldsymbol{e} = \\boldsymbol{y} -\\hat{\\boldsymbol{y}}\\) can be interpreted as the vertical distance between the regression slope and the observation. This is illustrated in the following graph. For each observation \\(y_i\\), a vertical line at distance \\(e_i\\) is drawn from the prediction \\(\\hat{y}_i\\). Side remark: graphs and table should always be properly labelled (including units). The last line of the call to plot contains cosmetic options that alter the display of the scatterplot — you can check for yourself the effects of removing any (all) of these additional commands. plot(mpg ~ wt, xlab = &quot;weight (1000 lbs)&quot;, ylab = &quot;Fuel consumption (miles/US gallon)&quot;, main = &quot;Fuel consumption of automobiles, 1974 Motor Trend&quot;, data = mtcars, bty = &quot;l&quot;, pch = 20, ylim = c(0, 35), xlim = c(0, 6)) #options to tweak the display #Line of best linear fit abline(a = beta_hat[1], b = beta_hat[2]) #Residuals are vertical distance from line to for(i in 1:nrow(X)){ segments(x0 = wt[i], y0 = fitted[i], y1 = fitted[i] + res[i], col = 2) } The same scatterplot, this time using ggplot2. library(ggplot2, warn.conflicts = FALSE, quietly = TRUE) #Create data frame with segments vlines &lt;- data.frame(x1 = mtcars$wt, y1 = fitted, y2 = fitted + res) ggg &lt;- ggplot(mtcars, aes(wt, mpg)) + geom_point() + labs(x = &quot;weight (1000 lbs)&quot;, y = &quot;Fuel consumption (miles/US gallon)&quot;, title = &quot;Fuel consumption of automobiles, 1974 Motor Trend&quot;) + geom_segment(aes(x = x1, y = y1, xend = x1, yend = y2, color = &quot;red&quot;), data = vlines, show.legend = FALSE) + geom_abline(slope = beta_hat[2], intercept = beta_hat[1]) print(ggg) "],
["interpretation.html", "2.3 Interpretation", " 2.3 Interpretation If the regression model is \\[y_i = \\beta_0 + \\mathrm{x}_{i1}\\beta_1 + \\mathrm{x}_{i2}\\beta_2 + \\varepsilon_i,\\] the interpretation of \\(\\beta_1\\) in the linear model is as follows: a unit increase in \\(x\\) leads to \\(\\beta_1\\) units increase in \\(y\\), everything else (i.e., \\(\\mathrm{x}_{i2}\\)) being held constant. For the mtcars regression above, an increase of the weight of the car of 1000 pounds leads to an average decrease of 5.34 miles per US gallon in distance covered by the car. We could easily get an equivalent statement in terms of increase of the car fuel consumption for a given distance. "],
["the-lm-function.html", "2.4 The lm function", " 2.4 The lm function The function lm is the workshorse for fitting linear models. It takes as input a formula: suppose you have a data frame containing columns x (a regressor) and y (the regressand); you can then call lm(y ~ x) to fit the linear model \\(y = \\beta_0 + \\beta_1x + \\varepsilon\\). The explanatory variable y is on the left hand side, while the right hand side should contain the predictors, separated by a + sign if there are more than one. If you provide the data frame name using data, then the shorthand y ~ . fits all the columns of the data frame (but y) as regressors. To fit higher order polynomials or transformations, use the I function to tell R to interpret the input “as is”. Thus, lm(y~x+I(x^2)), would fit a linear model with design matrix \\((\\boldsymbol{1}_n, \\mathbf{x}^\\top, \\mathbf{x}^2)^\\top\\). A constant is automatically included in the regression, but can be removed by writing -1 or +0 on the right hand side of the formula. # The function lm and its output fit &lt;- lm(mpg ~ wt, data = mtcars) fit_summary &lt;- summary(fit) The lm output will display OLS estimates along with standard errors, \\(t\\) values for the Wald test of the hypothesis \\(\\mathrm{H}_0: \\beta_i=0\\) and the associated \\(P\\)-values. Other statistics and information about the sample size, the degrees of freedom, etc., are given at the bottom of the table. Many methods allow you to extract specific objects. For example, the functions coef, resid, fitted, model.matrix will return \\(\\hat{\\boldsymbol{\\beta}}\\), \\(\\boldsymbol{e}\\), \\(\\hat{\\boldsymbol{y}}\\) and \\(\\mathbf{X}\\), respectively. names(fit) ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; ## [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; ## [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; names(fit_summary) ## [1] &quot;call&quot; &quot;terms&quot; &quot;residuals&quot; &quot;coefficients&quot; ## [5] &quot;aliased&quot; &quot;sigma&quot; &quot;df&quot; &quot;r.squared&quot; ## [9] &quot;adj.r.squared&quot; &quot;fstatistic&quot; &quot;cov.unscaled&quot; "]
]

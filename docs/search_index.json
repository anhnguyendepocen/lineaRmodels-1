[
["index.html", "lineaRmodels Preliminary remarks", " lineaRmodels Léo Belzile version of 2018-10-12 Preliminary remarks This is a web complement to MATH 341 (Linear Models), a first regression course for EPFL mathematicians. We shall use the R programming language througout the course (as it is free and it is used in other statistics courses at EPFL). Visit the R-project website to download the program. The most popular graphical cross-platform front-end is RStudio Desktop. R is an object-oriented interpreted language. It differs from usual programming languages in that it is designed for interactive analyses. Since R is not a conventional programming language, my teaching approach will be learning-by-doing. The benefit of using Rmarkdown is that you see the output directly and you can also copy the code. "],
["introduction.html", "1 Introduction", " 1 Introduction You can find several introductions to R online. Have a look at the R manuals or better at contributed manuals. A nice official reference is An introduction to R. You may wish to look up the following chapters of the R language definition (Evaluation of expressions and part of the Objects chapter). If you favor online courses, Data Camp offers a free introduction to R. "],
["basics-of-r.html", "1.1 Basics of R", " 1.1 Basics of R 1.1.1 Help Help can be accessed via help or simply ?. If you do not know what to query, use ?? in front of a string, delimited by captions &quot; &quot; as in ??&quot;Cholesky decomposition&quot;. Help is your best friend if you don’t know what a function does, what are its arguments, etc. 1.1.2 Basic commands Basic R commands are fairly intuitive, especially if you want to use R as a calculator. Elementary functions such as sum, min, max, sqrt, log, exp, etc., are self-explanatory. Some unconventional features of the language: Use &lt;- to assign to a variable, and = for matching arguments inside functions Indexing in R starts at 1, not zero. Most functions in R are vectorized, so avoid loops as much as possible. Integers are obtained by appending L to the number, so 2L is an integer and 2 a double. Besides integers and doubles, the common types are - logicals (TRUE and FALSE); - null pointers (NULL), which can be assigned to arguments; - missing values, namely NA or NaN. These can also be obtained a result of invalid mathematical operations such as log(-2). The above illustrates a caveat of R: invalid calls will often returns something rather than an error. It is therefore good practice to check that the output is sensical. 1.1.3 Linear algebra in R R is an object oriented language, and the basic elements in R are (column) vector. Below is a glossary with some useful commands for performing basic manipulation of vectors and matrix operations: c as in _c_oncatenates creates a vector cbind (rbind) binds column (row) vectors matrix and vector are constructors diag creates a diagonal matrix (by default with ones) t is the function for transpose solve performs matrix inversion %*% is matrix multiplication, * is element-wise multiplication crossprod(A, B) calculates the cross-product \\(\\mathbf{A}^\\top\\mathbf{B}\\), t(A) %*% B, of two matrices A and B. eigen/chol/qr/svd perform respectively an eigendecomposition/Cholesky/QR/singular value decomposition of a matrix rep creates a vector of duplicates, seq a sequence. For integers \\(i\\), \\(j\\) with \\(i&lt;j\\), i:j generates the sequence \\(i, i+1, \\ldots, j-1, j\\). Subsetting is fairly intuitive and general; you can use vectors, logical statements. For example, if x is a vector, then x[2] returns the second element x[-2] returns all but the second element x[1:5] returns the first five elements x[(length(x) - 5):length(x)] returns the last five elements x[c(1, 2, 4)] returns the first, second and fourth element x[x &gt; 3] return any element greater than 3. Possibly an empty vector of length zero! x[ x &lt; -2 | x &gt; 2] multiple logical conditions. which(x == max(x)) index of elements satisfying a logical condition. For a matrix x, subsetting now involves dimensions: [1,2] returns the element in the first row, second column. x[,2] will return all of the rows, but only the second column. For lists, you can use [[ for subsetting by index or the $ sign by names. 1.1.4 Packages The great strength of R comes from its contributed libraries (called packages), which contain functions and datasets provided by third parties. Some of these (base, stats, graphics, etc.) are loaded by default whenever you open a session. To install a package from CRAN, use install.packages(&quot;package&quot;), replacing package by the package name. Once installed, packages can be loaded using library(package); all the functions in package will be available in the environment. There are drawbacks to loading packages: if an object with the same name from another package is already present in your environment, it will be hidden. Use the double-colon operator :: to access a single object from an installed package (package::object). "],
["week1.html", "1.2 Tutorial 1", " 1.2 Tutorial 1 1.2.1 Datasets datasets are typically stored inside a data.frame, a matrix-like object whose columns contain the variables and the rows the observation vectors. The columns can be of different types (integer, double, logical, character), but all the column vectors must be of the same length. Variable names can be displayed by using names(faithful). Individual columns can be accessed using the column name using the $ operator. For example, faithful$eruptions will return the first column of the faithful dataset. To load a dataset from an (installed) R package, use the command data with the name of the package as an argument (must be a string). The package datasets is loaded by default whenever you open R, so these are always in the search path. The following functions can be useful to get a quick glimpse of the data: summary provides descriptive statistics for the variable. str provides the first few elements with each variable, along with the dimension head (tail) prints the first (last) \\(n\\) lines of the object to the console (default is \\(n=6\\)). We start by loading a dataset of the Old Faithful Geyser of Yellowstone National park and looking at its entries. # Load Old faithful dataset data(faithful, package = &quot;datasets&quot;) # Query the database for documentation ?faithful # look at first entries head(faithful) ## eruptions waiting ## 1 3.600 79 ## 2 1.800 54 ## 3 3.333 74 ## 4 2.283 62 ## 5 4.533 85 ## 6 2.883 55 str(faithful) ## &#39;data.frame&#39;: 272 obs. of 2 variables: ## $ eruptions: num 3.6 1.8 3.33 2.28 4.53 ... ## $ waiting : num 79 54 74 62 85 55 88 85 51 85 ... # What kind of object is faithful? class(faithful) ## [1] &quot;data.frame&quot; Other common classes of objects: - matrix: an object with attributes dim, ncol and nrow in addition to length, which gives the total number of elements. - array: a higher dimensional extension of matrix with arguments dim and dimnames. - list: an unstructured class whose elements are accessed using double indexing [[ ]] and elements are typically accessed using $ symbol with names. To delete an element from a list, assign NULL to it. - data.frame is a special type of list where all the elements are vectors of potentially different type, but of the same length. 1.2.2 Graphics The faithful dataset consists of two variables: the regressand waiting and the regressor eruptions. One could postulate that the waiting time between eruptions will be smaller if the eruption time is small, since pressure needs to build up for the eruption to happen. We can look at the data to see if there is a linear relationship between the variables. An image is worth a thousand words and in statistics, visualization is crucial. Scatterplots are produced using the function plot. You can control the graphic console options using par — see ?plot and ?par for a description of the basic and advanced options available. Once plot has been called, you can add additional observations as points (lines) to the graph using point (lines) in place of plot. If you want to add a line (horizontal, vertical, or with known intercept and slope), use the function abline. Other functions worth mentioning at this stage: boxplot creates a box-and-whiskers plot hist creates an histogram, either on frequency or probability scale (option freq = FALSE). breaks control the number of bins. rug adds lines below the graph indicating the value of the observations. pairs creates a matrix of scatterplots, akin to plot for data frame objects. There are two options for basic graphics: the base graphics package and the package ggplot2. The latter is a more recent proposal that builds on a modular approach and is more easily customizable — I suggest you stick to either and ggplot2 is a good option if you don’t know R already, as the learning curve will be about the same. Even if the display from ggplot2 is nicer, this is no excuse for not making proper graphics. Always label the axis and include measurement units! # Scatterplots # Using default R commands plot(waiting ~ eruptions, data = faithful, xlab = &quot;Eruption time (in min.)&quot;, ylab = &quot;Waiting time between eruptions (in min.)&quot;, main = &quot;Old Faithful Geyser Data&quot;) #using the grammar of graphics (more modular) #install.packages(&quot;ggplot2&quot;) #do this once only library(ggplot2) ggplot2::ggplot(data = faithful, aes(x = eruptions, y = waiting)) + geom_point() + labs(title = &quot;Old Faithful Geyser Data&quot;, x = &quot;Eruption time (in min.)&quot;, y = &quot;Waiting time between eruptions (in min.)&quot;) A simple linear model of the form \\[y_i = \\beta_0 + \\beta_1 \\mathrm{x}_i + \\varepsilon_i,\\] where \\(\\varepsilon_i\\) is a noise variable with expectation zero and \\(\\mathbf{x} = \\mathsf{eruptions}\\) and \\(\\boldsymbol{y} = \\mathsf{waiting}\\). We first create a matrix with a column of \\(\\mathbf{1}_n\\) for the intercept. We bind vectors by column (cbind) into a matrix, recycling arguments if necessary. Use $ to obtain a column of the data frame based on the name of the variable (partial matching is allowed, e.g., faithful$er is equivalent to faithful$eruptions in this case). ## Manipulating matrices n &lt;- nrow(faithful) p &lt;- ncol(faithful) y &lt;- faithful$waiting X &lt;- cbind(1, faithful$eruptions) 1.2.3 Projection matrices Recall that \\(\\mathbf{H}_{\\mathbf{X}} \\equiv \\mathbf{X}(\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\) is the orthogonal projection matrix onto \\(\\mathsf{span}(\\mathbf{X})\\). The latter has \\(p=2\\) eigenvalues equal to 1, is an \\(n \\times n\\) matrix of rank \\(p\\), is symmetric and idempotent. We can verify the properties of \\(\\mathbf{H}_{\\mathbf{X}}\\) numerically. Whereas we will frequently use == to check for equality of booleans, the latter should be avoided for comparisons because computer arithmetic is exact only in base 2. For example, 1/10 + 2/10 - 3/10 == 0 will return FALSE, whereas all.equal(1/10 + 2/10 - 3/10, 0) will return TRUE. Use all.equal to check for equalities. Hx &lt;- X %*% solve(crossprod(X)) %*% t(X) # Create projection matrix onto complement # `diag(n)` is the n by n identity matrix Mx &lt;- diag(n) - Hx #Check that projection leaves X invariant isTRUE(all.equal(X, Hx %*% X)) ## [1] TRUE #Check that orthogonal projection maps X to zero matrix of dimension (n, p) isTRUE(all.equal(matrix(0, nrow = n, ncol = p), Mx %*% X)) ## [1] TRUE #Check that the matrix Hx is idempotent isTRUE(all.equal(Hx %*% Hx, Hx)) ## [1] TRUE #Check that the matrix Hx is symmetric isTRUE(all.equal(t(Hx), Hx)) ## [1] TRUE #Check that only a two eigenvalue are 1 and the rest are zero isTRUE(all.equal(eigen(Hx, only.values = TRUE)$values, c(rep(1, p), rep(0, n - p)))) ## [1] TRUE #Check that the matrix has rank p isTRUE(all.equal(Matrix::rankMatrix(Hx), p, check.attributes = FALSE)) ## [1] TRUE "],
["exercises.html", "1.3 Exercises", " 1.3 Exercises 1.3.1 Auto dataset Install the R package ISLR and load the dataset Auto. Be careful, as R is case-sensitive. Query the help file for information about the dataset. Look at the first lines of Auto Create an explanatory variable x with horsepower and mileage per gallon as response y. Create a scatterplot of y against x. Is there evidence of a linear relationship between the two variables? Append a column vector of ones to x and create a projection matrix. Check that the resulting projection matrix is symmetric and idempotent. 1.3.2 Oblique projections (exercise 1.4) Suppose that \\(\\mathsf{span}(\\mathbf{X}) \\neq \\mathsf{span}(\\mathbf{W})\\), that both \\(\\mathbf{X}\\) and \\(\\mathbf{W}\\) are full-rank \\(n \\times p\\) matrices such that \\(\\mathbf{X}^\\top\\mathbf{W}\\) and \\(\\mathbf{W}^\\top\\mathbf{X}\\) are invertible. An oblique projection matrix is of the form \\(\\mathbf{P}\\equiv\\mathbf{X}(\\mathbf{W}^\\top\\mathbf{X})^{-1}\\mathbf{W}^\\top\\) and appears in instrumental variable regression. The oblique projection is such that \\(\\mathrm{im}(\\mathbf{P})=\\mathsf{span}(\\mathbf{X})\\), but \\(\\mathrm{im}(\\mathbf{I}-\\mathbf{P})=\\mathsf{span}(\\mathbf{W}^\\perp)\\). This fact is illustrated below. We consider two non-parallel vectors in \\(\\mathbb{R}^2\\), \\(\\mathbf{X}\\) and \\(\\mathbf{W}\\). #Create two vectors (non-parallel) x &lt;- c(1, 2) w &lt;- c(-1, 0.1) #Create oblique projection matrix P &lt;- x %*% solve(t(w) %*% x) %*% t(w) isTRUE(all.equal((P %*% P), P)) #P is idempotent ## [1] TRUE P - t(P) #but not symmetric ## [,1] [,2] ## [1,] 0.000 -2.625 ## [2,] 2.625 0.000 The figure below shows the projection of a third vector \\(\\mathbf{v}\\) (non-parallel to \\(\\mathbf{X}\\) or \\(\\mathbf{W}\\)) onto the span of \\(\\mathbf{P}\\) (blue), \\(\\mathbf{P}^\\top\\) (red), \\(\\mathbf{I}_2-\\mathbf{P}\\) (dashed cyan) and \\(\\mathbf{I}_2-\\mathbf{P}^\\top\\) (dashed orange). The circles indicate the vectors \\(\\mathbf{W}\\) (red) and \\(\\mathbf{X}\\) (blue) on the plane. Notice that \\(\\mathbf{I}_2-\\mathbf{P}^\\top \\perp \\mathbf{P}\\), whereas \\(\\mathbf{I}_2-\\mathbf{P} \\perp \\mathbf{P}^\\top\\). "],
["summary-of-week-1.html", "1.4 Summary of week 1", " 1.4 Summary of week 1 Let \\(\\mathbf{X}\\) be an \\(n \\times p\\) full-rank matrix (\\(p &lt;n\\)). An \\(n \\times n\\) orthogonal projection matrix \\(\\mathbf{H}\\) projects on to \\(\\mathcal{V} \\subseteq \\mathbb{R}^n\\), meaning \\(\\mathbf{Hv} \\in \\mathcal{V}\\) for any \\(\\mathbf{v} \\in \\mathbb{R}^n\\); is idempotent, meaning \\(\\mathbf{H} = \\mathbf{HH}\\); is symmetric, meaning \\(\\mathbf{H} = \\mathbf{H}^\\top\\). The projection matrix \\(\\mathbf{H}\\) is unique; if \\(\\mathcal{V} = {\\mathsf{span}}(\\mathbf{X})\\), then \\[\\mathbf{H}_{\\mathbf{X}} = \\mathbf{X}(\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top.\\] Since \\(\\mathbf{X}: \\mathbb{R}^n \\to \\mathbb{R}^p\\), \\(\\mathbf{H}_{\\mathbf{X}}\\) has rank \\(p\\). The orthogonal complement \\(\\mathbf{M}_{\\mathbf{X}}\\equiv \\mathbf{I}_n - \\mathbf{H}_{\\mathbf{X}}\\) projects onto \\({\\mathsf{span}}^{\\perp}(\\mathbf{X})\\). "],
["computational-considerations.html", "2 Computational considerations", " 2 Computational considerations In this tutorial, we will explore some basic R commands and illustrate their use on the Auto dataset (Auto) from the ISLR package. "],
["calculation-of-least-square-estimates.html", "2.1 Calculation of least square estimates", " 2.1 Calculation of least square estimates Consider as usual \\(\\boldsymbol{y}\\) and \\(n\\)-vector of response variables and a full-rank \\(n \\times p\\) design matrix \\(\\mathbf{X}\\). We are interested in finding the ordinary least square coefficient \\(\\hat{\\boldsymbol{\\beta}}\\), the fitted values \\(\\hat{\\boldsymbol{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\) and the residuals \\(\\boldsymbol{e} = \\boldsymbol{y} - \\mathbf{X}\\boldsymbol{\\beta}\\). Whereas orthogonal projection matrices are useful for theoretical derivations, they are not used for computations. Building \\(\\mathbf{H}_{\\mathbf{X}}\\) involves a matrix inversion and the storage of an \\(n \\times n\\) matrix. In Exercise series 2, we looked at two matrix decompositions: a singular value decomposition (SVD) and a QR decomposition. These are more numerically stable than using the normal equations \\((\\mathbf{X}^\\top\\mathbf{X})\\boldsymbol{\\beta} = \\mathbf{X}^\\top\\boldsymbol{y}\\) (the condition number of the matrix \\(\\mathbf{X}^\\top\\mathbf{X}\\) is the square of that of \\(\\mathbf{X}\\) — more on this later). Optional material: for more details about the complexity and algorithms underlying the different methods, the reader is referred to these notes of Lee. 2.1.1 Normal equations The following simply illustrates what has been derived in Exercise series 2. You will probably never use these commands, as R has devoted functions that are coded more efficiently. We can compute first the ordinary least square estimates using the formula \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\boldsymbol{y}\\). data(Auto, package = &quot;ISLR&quot;) y &lt;- Auto$mpg X &lt;- cbind(1, Auto$horsepower) n &lt;- nrow(X) p &lt;- ncol(X) # Estimation of betahat: XtX &lt;- crossprod(X) Xty &lt;- crossprod(X, y) # Solve normal equations betahat &lt;- as.vector(solve(XtX, Xty)) #same as betahat &lt;- solve(t(X) %*% X) %*% t(X) %*% y 2.1.2 Singular value decomposition The SVD decomposition in R returns a list with elements u, d and v. u is the orthonormal \\(n \\times p\\) matrix, d is a vector containing the diagonal elements of \\(\\mathbf{D}\\) and v is the \\(p \\times p\\) orthogonal matrix. Recall that the decomposition is \\[\\mathbf{X} = \\mathbf{UDV}^\\top\\] and that \\(\\mathbf{VV}^\\top= \\mathbf{V}^\\top\\mathbf{V}=\\mathbf{U}^\\top\\mathbf{U}=\\mathbf{I}_p\\). The matrix \\(\\mathbf{D}\\) contains the singular values of \\(\\mathbf{X}\\), and the diagonal elements \\(\\mathrm{d}_{ii}^2\\) corresponds to the (ordered) eigenvalues of \\(\\mathbf{X}^\\top\\mathbf{X}\\). svdX &lt;- svd(X) # Projection matrix Hx &lt;- tcrossprod(svdX$u) # t(U) %*% U gives p by p identity matrix all.equal(crossprod(svdX$u), diag(p)) ## [1] TRUE # V is an orthogonal matrix all.equal(tcrossprod(svdX$v), diag(p)) ## [1] TRUE all.equal(crossprod(svdX$v), diag(p)) ## [1] TRUE # D contains singular values all.equal(svdX$d^2, eigen(XtX, only.values = TRUE)$values) ## [1] TRUE # OLS coefficient from SVD betahat_svd &lt;- c(svdX$v %*% diag(1/svdX$d) %*% t(svdX$u) %*% y) all.equal(betahat_svd, betahat) ## [1] TRUE 2.1.3 QR decomposition R uses a QR-decomposition to calculate the OLS. There are specific functions to return coefficients, fitted values and residuals. One can also obtain the \\(n \\times p\\) matrix \\(\\mathbf{Q}_1\\) and the upper triangular \\(p \\times p\\) matrix \\(\\mathbf{R}\\) from the thinned QR decomposition, \\[\\mathbf{X} = \\mathbf{Q}_1\\mathbf{R}.\\] qrX &lt;- qr(X) Q1 &lt;- qr.Q(qrX) R &lt;- qr.R(qrX) # Compute betahat from QR betahat_qr1 &lt;- qr.coef(qrX, y) #using built-in function betahat_qr2 &lt;- c(backsolve(R, t(Q1) %*% y)) #manually all.equal(betahat, betahat_qr1, check.attributes = FALSE) ## [1] TRUE all.equal(betahat, betahat_qr2, check.attributes = FALSE) ## [1] TRUE # Compute residuals qre &lt;- qr.resid(qrX, y) all.equal(qre, c(y - X %*% betahat), check.attributes = FALSE) ## [1] TRUE # Compute fitted values qryhat &lt;- qr.fitted(qrX, y) all.equal(qryhat, c(X %*% betahat), check.attributes = FALSE) ## [1] TRUE # Compute orthogonal projection matrix qrHx &lt;- tcrossprod(Q1) all.equal(qrHx, Hx) ## [1] TRUE "],
["parameter-estimation.html", "2.2 Parameter estimation", " 2.2 Parameter estimation We are now ready to fit a simple linear model with an intercept and a linear effect for the weight, \\[ \\texttt{mpg}_i = \\beta_0 + \\texttt{hp}_i\\beta_1 +\\varepsilon_i.\\] We form the design matrix \\((\\boldsymbol{1}_n^\\top, \\texttt{hp}^\\top)^\\top\\) and the vector of regressand \\(\\texttt{mpg}\\), then proceed with calculating the OLS coefficients \\(\\hat{\\boldsymbol{\\beta}}\\), the hat matrix \\(\\mathbf{H}_{\\mathbf{X}}\\), the fitted values \\(\\hat{\\boldsymbol{y}}\\) and the residuals \\(\\boldsymbol{e}\\). #Design matrix hp &lt;- Auto$horsepower X &lt;- cbind(1, Auto$horsepower) mpg &lt;- Auto$mpg #OLS estimates XtXinv &lt;- solve(crossprod(X)) beta_hat &lt;- c(XtXinv %*% t(X) %*% mpg) #Form orthogonal projection matrix Hmat &lt;- X %*% XtXinv %*% t(X) #Create residuals and fitted values fitted &lt;- Hmat %*% mpg res &lt;- mpg - fitted fitted &lt;- Hmat %*% mpg The residuals \\(\\boldsymbol{e} = \\boldsymbol{y} -\\hat{\\boldsymbol{y}}\\) can be interpreted as the vertical distance between the regression slope and the observation. For each observation \\(y_i\\), a vertical line at distance \\(e_i\\) is drawn from the prediction \\(\\hat{y}_i\\). plot(mpg ~ horsepower, data = Auto, xlab = &quot;Power of engine (hp)&quot;, ylab = &quot;Fuel economy (miles/US gallon)&quot;, main = &quot;Fuel economy of automobiles&quot;, # the subsequent commands for `plot` tweak the display # check for yourself the effect of removing them # bty = &quot;l&quot; gives L shaped graphical windows (not boxed) # pch = 20 gives full dots rather than empty circles for points bty = &quot;l&quot;, pch = 20) #Line of best linear fit abline(a = beta_hat[1], b = beta_hat[2]) #Residuals are vertical distance from line to for(i in 1:nrow(X)){ segments(x0 = hp[i], y0 = fitted[i], y1 = fitted[i] + res[i], col = 2) } The same scatterplot, this time using ggplot2. library(ggplot2, warn.conflicts = FALSE, quietly = TRUE) #Create data frame with segments vlines &lt;- data.frame(x1 = hp, y1 = fitted, y2 = fitted + res) ggg &lt;- ggplot(Auto, aes(x = horsepower, y = mpg)) + geom_point() + labs(x = &quot;Power of engine (hp)&quot;, y = &quot;Fuel economy (miles/US gallon)&quot;, title = &quot;Fuel economy of automobiles&quot;) + geom_segment(aes(x = x1, y = y1, xend = x1, yend = y2, color = &quot;red&quot;), data = vlines, show.legend = FALSE) + geom_abline(slope = beta_hat[2], intercept = beta_hat[1]) print(ggg) "],
["interpretation-of-the-coefficients.html", "2.3 Interpretation of the coefficients", " 2.3 Interpretation of the coefficients If the regression model is \\[y_i = \\beta_0 + \\mathrm{x}_{i1}\\beta_1 + \\mathrm{x}_{i2}\\beta_2 + \\varepsilon_i,\\] the interpretation of \\(\\beta_1\\) in the linear model is as follows: a unit increase in \\(x\\) leads to \\(\\beta_1\\) units increase in \\(y\\), everything else (i.e., \\(\\mathrm{x}_{i2}\\)) being held constant. For the Auto regression above, an increase of the power of the engine by one horsepower leads to an average decrease of 0.16 miles per US gallon in distance covered by the car. We could easily get an equivalent statement in terms of increase of the car fuel consumption for a given distance. "],
["the-lm-function.html", "2.4 The lm function", " 2.4 The lm function The function lm is the workshorse for fitting linear models. It takes as input a formula: suppose you have a data frame containing columns x (a regressor) and y (the regressand); you can then call lm(y ~ x) to fit the linear model \\(y = \\beta_0 + \\beta_1x + \\varepsilon\\). The explanatory variable y is on the left hand side, while the right hand side should contain the predictors, separated by a + sign if there are more than one. If you provide the data frame name using data, then the shorthand y ~ . fits all the columns of the data frame (but y) as regressors. To fit higher order polynomials or transformations, use the I function to tell R to interpret the input “as is”. Thus, lm(y~x+I(x^2)), would fit a linear model with design matrix \\((\\boldsymbol{1}_n, \\mathbf{x}^\\top, \\mathbf{x}^2)^\\top\\). A constant is automatically included in the regression, but can be removed by writing -1 or +0 on the right hand side of the formula. # The function lm and its output fit &lt;- lm(mpg ~ horsepower + I(horsepower^2), data = Auto) fit_summary &lt;- summary(fit) The lm output will display OLS estimates along with standard errors, \\(t\\) values for the Wald test of the hypothesis \\(\\mathrm{H}_0: \\beta_i=0\\) and the associated \\(P\\)-values. Other statistics and information about the sample size, the degrees of freedom, etc., are given at the bottom of the table. Many methods allow you to extract specific objects. For example, the functions coef, resid, fitted, model.matrix will return \\(\\hat{\\boldsymbol{\\beta}}\\), \\(\\boldsymbol{e}\\), \\(\\hat{\\boldsymbol{y}}\\) and \\(\\mathbf{X}\\), respectively. names(fit) ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; ## [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; ## [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; names(fit_summary) ## [1] &quot;call&quot; &quot;terms&quot; &quot;residuals&quot; &quot;coefficients&quot; ## [5] &quot;aliased&quot; &quot;sigma&quot; &quot;df&quot; &quot;r.squared&quot; ## [9] &quot;adj.r.squared&quot; &quot;fstatistic&quot; &quot;cov.unscaled&quot; "],
["the-hyperplane-of-fitted-values.html", "2.5 The hyperplane of fitted values", " 2.5 The hyperplane of fitted values In class, we presented a linear model for the Auto dataset of the form \\[\\mathsf{mpg}_i = \\beta_0 + \\beta_1 \\mathsf{hp}_i + \\beta_2 \\mathsf{hp}_i^2 + \\varepsilon_i\\] and claimed this was a linear model. This is indeed true because we can form the design matrix \\([\\mathbf{1}_n, \\mathsf{hp}, \\mathsf{hp}^2]\\) and obtain coefficients \\({\\hat{\\boldsymbol{\\beta}}}\\). The graphical depiction is counterintuitive. This quadratic curve is nothing like an hyperplane! Let \\({\\boldsymbol{y}} \\equiv \\texttt{mpg}\\), \\(\\mathsf{x} = \\texttt{hp}\\) and \\(\\mathsf{z} = \\texttt{hp}^2\\). But recall that we are working in three dimensions (the intercept gives the height of the hyperplane) and the coordinates of our hyperplane are \\[\\beta_0 + \\beta_1x-y +\\beta_2z =0.\\] However, the observations will always be such that \\(z = x^2\\), so our fitted values will lie on a one-dimensional subspace of this hyperplane. The following 3D depiction hopefully captures this better and shows the fitted hyperplane along with the line on which all the (\\(x_i, z_i\\)) observations lie. "],
["centered-coefficient-of-determination.html", "2.6 (Centered) coefficient of determination", " 2.6 (Centered) coefficient of determination Recall the decomposition of observations into fitted and residual vectors, \\[\\boldsymbol{y} = (\\boldsymbol{y} - {\\mathbf{X}}{\\hat{\\boldsymbol{\\beta}}}) + {\\mathbf{X}}{\\hat{\\boldsymbol{\\beta}}}= {\\boldsymbol{e}} + \\hat{{\\boldsymbol{y}}}\\] where \\({\\boldsymbol{e}} \\equiv {\\mathbf{M}}_{{\\mathbf{X}}}{\\boldsymbol{y}} \\perp \\hat{{\\boldsymbol{y}}} \\equiv {\\mathbf{H}}_{{\\mathbf{X}}}{\\boldsymbol{y}}\\). The centered coefficient of determination, \\(R^2_c\\) measures the proportion of variation explained by the centered fitted values relative to the centered observations, i.e., \\[ R^2_c = \\frac{\\|\\hat{{\\boldsymbol{y}}}-\\bar{y}\\mathbf{1}_n\\|^2}{\\|{\\boldsymbol{y}}-\\bar{y}\\mathbf{1}_n\\|^2}=\\frac{\\|\\hat{{\\boldsymbol{y}}}\\|^2-\\|\\bar{y}\\mathbf{1}_n\\|^2}{\\|{\\boldsymbol{y}}\\|^2-\\|\\bar{y}\\mathbf{1}_n\\|^2}.\\] since the vectors \\(\\bar{y}\\mathbf{1}_n \\perp \\hat{{\\boldsymbol{y}}}-\\bar{y}\\mathbf{1}_n\\). Provided that \\(\\mathbf{1}_n \\in {\\mathsf{span}}({\\mathbf{X}})\\), it is obvious that the fitted values \\(\\hat{{\\boldsymbol{y}}}\\) are invariant to linear transformations of the covariates \\(\\mathbf{X}\\) (by which I mean you can transform the design matrix column by column, with \\(\\mathbf{x}_i \\mapsto \\alpha_i+\\mathbf{x}_i\\gamma_i\\) for \\(i=1, \\ldots, p\\)). Multiplicative changes in \\({\\boldsymbol{y}}\\) lead to an equivalent change in \\({\\boldsymbol{e}}\\) and \\(\\hat{{\\boldsymbol{y}}}\\). However, location-changes in \\({\\boldsymbol{y}}\\) are only reflected in \\(\\hat{{\\boldsymbol{y}}}\\) (they are absorbed by the intercept). This is why \\(R^2\\) is not invariant to location-changes in the response, since the ratio \\(\\|\\hat{{\\boldsymbol{y}}}\\|^2/\\|{\\boldsymbol{y}}\\|^2\\) increases to 1 if \\({{\\boldsymbol{y}}}\\mapsto {{\\boldsymbol{y}}}+ a \\mathbf{1}_n\\). This invariance is precisely the reason we dismissed \\(R^2\\). For example, a change of units from Farenheit to Celcius, viz. \\(T_c = 5 (T_F - 32)/9\\), leads to different values of \\(R^2\\): data(aatemp, package = &quot;faraway&quot;) plot(temp ~ year, data = aatemp, ylab = &quot;Temperature (in F)&quot;, bty = &quot;l&quot;) #Form design matrix and two response vectors yF &lt;- aatemp$temp n &lt;- length(yF) yC &lt;- 5/9*(aatemp$temp - 32) X &lt;- cbind(1, aatemp$year) # Obtain OLS coefficients and fitted values XtX &lt;- solve(crossprod(X)) beta_hat_F &lt;- XtX %*% crossprod(X, yF) abline(a = beta_hat_F[1], b = beta_hat_F[2]) beta_hat_C &lt;- XtX %*% crossprod(X, yC) fitted_F &lt;- X %*% beta_hat_F fitted_C &lt;- X %*% beta_hat_C # Compute coefficient of determination R2_F &lt;- sum(fitted_F^2)/sum(yF^2) R2_C &lt;- sum(fitted_C^2)/sum(yC^2) #Centered R^2 R2c_F &lt;- sum((fitted_F-mean(yF))^2)/sum((yF-mean(yF))^2) R2c_C &lt;- sum((fitted_C-mean(yC))^2)/sum((yC-mean(yC))^2) isTRUE(all.equal(R2c_F, R2c_C)) ## [1] TRUE The difference \\(R^2(F)-R^2(C)=\\) 0.00752 is small because the \\(R^2\\) value is very high, but the coefficient itself is also meaningless. In this example, \\(R^2(F)=\\) 0.9991, which seems to indicate excellent fit but in fact only 8.54% of the variability is explained by year and we do an equally good job by simply taking \\(\\hat{y}_i=\\bar{y}\\). \\(R^2_c\\) makes the comparison between the adjusted linear model and the null model with only a constant, which predicts each \\(y_i (i=1, \\ldots, n)\\) by the average \\(\\bar{y}\\). If \\(R^2_c\\) gives a very rough overview of how much explanatory power \\({\\mathbf{X}}\\) has, it is not a panacea. If we add new covariates in \\({\\mathbf{X}}\\), the value of \\(R^2_c\\) necessarily increases. In the most extreme scenario, we could add a set of \\(n-p\\) linearly independent vectors to \\({\\mathbf{X}}\\) and form a new design matrix \\(mX^*\\) with those. The fitted values from running a regression with \\({\\mathbf{X}}^*\\) will be exactly equal to the observations \\({\\boldsymbol{y}}\\) and thus \\(R^2_c=1\\). However, I hope it is clear that this model will not be useful. Overfitting leads to poor predictive performance; if we get a new set of \\(\\mathbf{x}_*\\), we would predict the unobserved \\(y_*\\) using its conditional average \\(\\mathbf{x}_i^*{\\hat{\\boldsymbol{\\beta}}}\\) and this estimate will be rubish if we included too many meaningless covariates. Other versions of \\(R^2_c\\) exist that include a penalty term for the number of covariates; these are not widely used and can be negative in extreme cases. We will cover better goodness-of-fit diagnostics later in the course. "],
["summary-of-week-2.html", "2.7 Summary of week 2", " 2.7 Summary of week 2 If \\(\\mathbf{X}\\) is an \\(n \\times p\\) design matrix containing covariates and \\(\\boldsymbol{Y}\\) is our response variable, we can obtain the ordinary least squares (OLS) coefficients for the linear model \\[\\boldsymbol{y} = \\mathbf{X}{\\boldsymbol{\\beta}}+ {\\boldsymbol{\\varepsilon}}, \\qquad \\mathrm{E}({\\boldsymbol{\\varepsilon}})=\\boldsymbol{0}_n,\\] by projecting \\(\\boldsymbol{y}\\) on to \\(\\mathbf{X}\\); it follows that \\[{\\mathbf{X}}\\hat{\\boldsymbol{\\beta}}={\\mathbf{X}}(\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top{{\\boldsymbol{y}}}\\] and \\[\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top{{\\boldsymbol{y}}}.\\] The dual interpretation (which is used for graphical diagnostics), is the row geometry: each row corresponds to an individual and the response is a \\(1\\) dimensional point. \\({\\hat{\\boldsymbol{\\beta}}}\\) describes the parameters of the hyperplane that minimizes the sum of squared Euclidean vertical distances between the fitted value \\(\\hat{y}_i\\) and the response \\(y_i\\). The problem is best written using vector-matrix notation, so \\[ \\mathrm{argmin}_{{\\boldsymbol{\\beta}}} \\sum_{i=1}^n (y_i- \\mathbf{x}_i{\\boldsymbol{\\beta}})^2 \\equiv \\mathrm{argmin}_{{\\boldsymbol{\\beta}}} ({\\boldsymbol{y}} - {\\mathbf{X}}{\\boldsymbol{\\beta}})^\\top({\\boldsymbol{y}}-{\\mathbf{X}}{\\boldsymbol{\\beta}}) \\equiv {\\boldsymbol{e}}^\\top{\\boldsymbol{e}}. \\] The solution to the OLS problem has a dual interpretation in the column geometry, in which we treat the vector of stacked observations \\((y_1, \\ldots, y_n)^\\top\\) (respectively the vertical distances \\((e_1, \\ldots, e_n)^\\top\\)) as elements of \\(\\mathbb{R}^n\\). There, the response \\({\\boldsymbol{y}}\\) space can be decomposed into fitted values \\(\\hat{{{\\boldsymbol{y}}}} \\equiv {\\mathbf{H}}_{{\\mathbf{X}}} = {\\mathbf{X}}{\\hat{\\boldsymbol{\\beta}}}\\) and residuals \\({\\boldsymbol{e}} = {\\mathbf{M}}_{{\\mathbf{X}}} = {\\boldsymbol{y}} - {\\mathbf{X}}{\\hat{\\boldsymbol{\\beta}}}\\). By construction, \\({\\boldsymbol{e}} \\perp \\hat{by}\\). We therefore get \\[{\\boldsymbol{y}} = \\hat{{\\boldsymbol{y}}} + {\\boldsymbol{e}}\\] and since these form a right-angled triangle, Pythagoras’ theorem can be used to show that \\(\\|{\\boldsymbol{y}}\\|^2 = \\|\\hat{{\\boldsymbol{y}}}\\|^2 + \\|{\\boldsymbol{e}}\\|^2.\\) "],
["exercises-1.html", "2.8 Exercises", " 2.8 Exercises 2.8.1 Prostate cancer dataset The following questions refer to the dataset prostate from the package ElemStatLearn. Briefly describe the dataset. Look at summaries of lbph. What likely value was imputed in places of zeros in `lbph} (before taking the logarithm)? Produce a plot of the pair of variables lcavol and lpsa on the log and on the original scale. Comment on the relationship between lcavol and lpsa. Fit a linear model using the log cancer volume as response variable, including a constant and the log prostate specific antigen as covariates. Obtain numerically the OLS estimates \\({\\hat{\\boldsymbol{\\beta}}}\\) of the parameters, the fitted values \\(\\hat{{\\boldsymbol{y}}}\\) and the residuals \\({\\boldsymbol{e}}\\) using the formulas given in class. Compare the quantities you obtained with the output of the function lm. Add the fitted regression line to the scatterplot of lcavol against lpsa. Interpret the changes in cancer volume (not the log cancer volume), including any units in your interpretations. Obtain the orthogonal projection matrix \\(\\bf{H}_{\\mathbf{X}}\\) and the OLS coefficients \\({\\hat{\\boldsymbol{\\beta}}}\\) using a SVD decomposition of \\({\\mathbf{X}}\\) (svd). Compute the \\(R^2_c\\) coefficient and compare with the one in summary output of the lm function. What can you say about the explanatory power of the covariate lpsa? "],
["solutions.html", "2.9 Solutions", " 2.9 Solutions The following questions refer to the dataset prostate from the package ElemStatLearn. Briefly describe the data set. Running ?ElemStatLearn::prostate gives the help file for the data set. Since we will be coming back to this example, detailed informations are provided below. This data set was extracted from Stamey, T.A., Kabalin, J.N., McNeal, J.E., Johnstone, I.M., Freiha, F., Redwine, E.A. and Yang, N. (1989) Prostate specific antigen in the diagnosis and treatment of adenocarcinoma of the prostate: II. radical prostatectomy treated patients, Journal of Urology 141(5), 1076–1083. This data set is described in Wakefield (2013), pp. 5-6. The data were collected on \\(n=97\\) men before radical prostatectomy, a major surgical operation that removes the entire prostate gland along with some surrounding tissue. In Stamey et al. (1989), prostate specific antigen (PSA) was proposed as a preoperative marker to predict the clinical stage of cancer. As well as modeling the stage of cancer as a function of PSA, the authors also examined PSA as a function of age and seven other histological and morphometric covariates. The BPH and capsular penetration variables originally contained zeros, and a small number was substituted before the log transform was taken. It is not clear from the original paper why the log transform was taken though PSA varies over a wide range, and so linearity of the mean model may be aided by the log transform. It is also not clear why the variable PGS45 was constructed. The data set contains the following variables: lcavol: log of cancer volume, measured in milliliters (cc). The area of cancer was measured from digitized images and multiplied by a thickness to produce a volume. lweight: log of the prostate weight, measured in grams. age: The age of the patient, in years. lbph: log of the amount of benign prostatic hyperplasia (BPH), a noncancerous enlargement of the prostate gland, as an area in a digitized image and reported in cm\\({}^2\\). svi: seminal vesicle invasion, a 0/1 indicator of whether prostate cancer cells have invaded the seminal vesicle. lcp: log of the capsular penetration, which represents the level of extension of cancer into the capsule (the fibrous tissue which acts as an outer lining of the prostate gland), measured as the linear extent of penetration, in cm. gleason: Gleason score, a measure of the degree of aggressiveness of the tumor. The Gleason grading system assigns a grade (1–5) to each of the two largest areas of cancer in the tissue samples with 1 being the least aggressive and 5 the most aggressive; the two grades are then added together to produce the Gleason score. pgg45: percentage of Gleason scores that are 4 or 5. lpsa: log of prostate specific antigen (PSA), a concentration measured in ng/m To load the data set, use #Install package if you get an error message #install.packages(&quot;ElemStatLearn&quot;) data(prostate, package = &quot;ElemStatLearn&quot;) ?ElemStatLearn::prostate attach(prostate) The command attach allows you to access column (variables) without using $ by adding the columns of the data frame to your work environment. Always detach the data once you are done with your analysis to avoid overriding or hidding variables. Look at summaries of lbph. What likely value was imputed in places of zeros in lbph (before taking the logarithm)? bph &lt;- exp(lbph) head(bph) #look up first elements ## [1] 0.25 0.25 0.25 0.25 0.25 0.25 min(bph) #return minimum ## [1] 0.25 hist(bph, main = &quot;Histogram&quot;, xlab = &quot;benign prostatic hyperplasia&quot;) rug(bph) #histogram, with lines below where the observations are It seems likely that in order to take a logarithm, zeros were changed to 0.25. As such, we have to be careful with the interpretation of this coefficient if we include bph in the regression. Produce a plot of the pair of variables lcavol and lpsa on the log and on the original scale. Comment on the relationship between lcavol and lpsa. par(mfrow = c(1, 2)) #graphical parameters: two graphs per window #Function plot is plot(x = , y = ) or plot(y ~ x) #this works for vectors! (error message otherwise) plot(exp(lcavol) ~ exp(lpsa), ylab = &quot;Cancer volume (milliliters per cc)&quot;, #y-axis label xlab = &quot;prostate specific antigen (ng/ml)&quot;, #x-axis label main = &quot;Prostate cancer dataset&quot;, #title bty = &quot;l&quot;, pch = 20) #bty: remove box, only x-y axis #pch: type of plotting symbol (small filled circle) plot(y = lcavol, x = lpsa, ylab = &quot;cancer volume (milliliters per cc), log scale&quot;, xlab = &quot;prostate specific antigen (ng/ml), log scale&quot;, main = &quot;Prostate cancer dataset&quot;, bty = &quot;l&quot;, pch = 20) hist(exp(lcavol), xlab = &quot;cancer volume (milliliters per cc)&quot;, main = &quot;Histogram&quot;) rug(exp(lcavol)) hist(exp(lpsa), xlab = &quot;prostate specific antigen (ng/ml)&quot;, main = &quot;Histogram&quot;) rug(exp(lpsa)) With ggplot2, the same graphs library(ggplot2) ggplot(data = prostate, aes(y = lcavol, x = lpsa)) + geom_point() + labs(x = &quot;prostate specific antigen (ng/ml), log scale&quot;, y = &quot;cancer volume (milliliters per cc), log scale&quot;, title = &quot;Prostate cancer dataset&quot;) ggplot(data = prostate, aes(y = exp(lcavol), x = exp(lpsa))) + geom_point() + labs(x = &quot;prostate specific antigen (ng/ml)&quot;, y = &quot;cancer volume (milliliters per cc)&quot;, title = &quot;Prostate cancer dataset&quot;) ggplot(data = prostate, aes(x = exp(lcavol))) + geom_histogram(bins = 30) + geom_rug() + labs(x = &quot;cancer volume (milliliters per cc)&quot;, title = &quot;Histogram&quot;) We can see that both variables are positive and positively skewed, so a log transform may lead to a more linear relationship, as indicated by the pairs plot. A multiplicative model on the original scale is thus reasonable. Fit a linear model using the log cancer volume as response variable, including a constant and the log prostate specific antigen as covariates. Obtain numerically the OLS estimates \\(\\hat{\\boldsymbol{\\beta}}\\) of the parameters, the fitted values \\(\\hat{\\boldsymbol{y}}\\) and the residuals \\(\\boldsymbol{e}\\) using the formulae given in class. fit &lt;- lm(lcavol ~ lpsa, data = prostate) summary(fit) ## ## Call: ## lm(formula = lcavol ~ lpsa, data = prostate) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.15949 -0.59384 0.05034 0.50826 1.67751 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.50858 0.19419 -2.619 0.0103 * ## lpsa 0.74992 0.07109 10.548 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8041 on 95 degrees of freedom ## Multiple R-squared: 0.5394, Adjusted R-squared: 0.5346 ## F-statistic: 111.3 on 1 and 95 DF, p-value: &lt; 2.2e-16 #Create response vector and design matrix y &lt;- lcavol X &lt;- cbind(1, lpsa) #Create function to compute coefs &quot;by hand&quot; coefs_vals &lt;- function(x, y){ c(solve(crossprod(x), crossprod(x, y))) } # Compute coefficients, fitted values and residuals beta_hat &lt;- coefs_vals(x = X, y = lcavol) yhat &lt;- c(X %*% beta_hat) e &lt;- y - yhat The function lm fits a linear model by least squares to a dataset. The function summary will return coefficient estimates, standard errors and various other statistics and print them in the console. The formula for lm must be of the form y ~, and any combination of the variables appearing on the right hand side of the ~ will be added as new columns of the design matrix. By default, the latter includes a column of ones. To remove it, use +0 or -1. If you have two covariates x1 and x2, the model x1+x2 will have for \\(i\\)th row \\((1, x_{i1}, x_{i2})\\), while the model x1+x2+x1:x2\\(\\equiv\\)x1*x2 will include an interaction term x1:x2. The latter just means product, so the \\(i\\)th row of the design matrix would be \\((1, x_{i1}, x_{i2}, x_{i1}x_{i2})\\). R will drop any collinear vectors, warn you and report NA in the summary output. Compare the quantities you obtained in the last question with the output of the function lm. beta_hat # equivalent to print(beta_hat) ## [1] -0.5085796 0.7499189 coef(fit) # coefficients from object of class `lm` ## (Intercept) lpsa ## -0.5085796 0.7499189 isTRUE(all.equal(beta_hat, coef(fit), check.attributes = FALSE)) ## [1] TRUE isTRUE(all.equal(c(yhat), fitted(fit), check.attributes = FALSE)) ## [1] TRUE isTRUE(all.equal(e, resid(fit), check.attributes = FALSE)) ## [1] TRUE Add the fitted regression line to the scatterplot of lcavol against lpsa . par(mfrow = c(1, 1)) plot(lcavol ~ lpsa, data = prostate, ylab = &quot;Cancer volume (milliliters per cc), log scale&quot;, xlab = &quot;prostate specific antigen (ng/ml), log scale&quot;, main = &quot;Prostate cancer dataset&quot;, bty = &quot;l&quot;, pch = 20) abline(fit, lwd = 2) #simply add regression line, lwd is line width ggplot(data = prostate, aes(y = lcavol, x = lpsa)) + geom_point() + labs(x = &quot;prostate specific antigen (ng/ml), log scale&quot;, y = &quot;cancer volume (milliliters per cc), log scale&quot;, title = &quot;Prostate cancer dataset&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) Interpret the changes in cancer volume (not the log cancer volume), including any units in your interpretations. The interpretation is as follows. We fit \\[\\log(\\texttt{cavol}_i) = \\beta_0 + \\beta_1 \\log(\\texttt{psa}_i) + \\varepsilon_i.\\] On the original scale, this translates into the multiplicative model \\(\\texttt{cavol}_i= \\exp^{\\beta_0}\\texttt{psa}_i^{\\beta_1}\\exp(\\varepsilon_i)\\). The effect of an increase of one ng/mll of prostate specific antigen depends on the specific level of \\(\\texttt{psa}\\), \\((\\texttt{psa}_1/\\texttt{psa}_2)^{\\beta_1}\\) for levels \\(\\texttt{psa}_1\\) and \\(\\texttt{psa}_2\\). For example, an increase of the PSA level from 5.25 ng/mll to 6.15 ng/mll leads an increase of the volume of cancer of prostate cancer of 1.13 milliliters per cubic centimeter. Using the results of Exercise 4.2, obtain the orthogonal projection matrix \\(\\mathbf{H}_{\\mathbf{X}}\\) and \\(\\hat{\\boldsymbol{\\beta}}\\) using a SVD decomposition (svd). Check your output. #Hat matrix Hmat &lt;- X %*% solve(crossprod(X)) %*% t(X) #SVD decomposition of X svdX &lt;- svd(X) #OLS coefficients beta_hat_svd &lt;- svdX$v %*% (t(svdX$u) %*% lcavol / svdX$d) Hmat_svd &lt;- tcrossprod(svdX$u) #Check that both quantities are equal all.equal(Hmat, Hmat_svd, check.attributes = FALSE) ## [1] TRUE #use check.attributes = FALSE #if you want to compare only the values #and not e.g. the column names all.equal(c(beta_hat_svd), beta_hat) ## [1] TRUE Compute the \\(R^2_c\\) coefficient and compare with the one in summary output of the lm function. What can you say about the explanatory power of the covariate lpsa ? R2c &lt;- sum((yhat-mean(y))^2)/sum((y-mean(y))^2) R2c_lm &lt;- summary(fit)$r.squared #this is centered version all.equal(R2c, R2c_lm) ## [1] TRUE #Detach prostate from environment detach(prostate) The value of \\(R^2_c\\) is about 0.54, so about half the variability can be explained by the model. There is reasonable explanatory power. Note that presence of cancer causes the prostate specific antigens to increase (not the other way around!). A linear model could nevertheless be sensible here if we wished to obtain a non-invasive detector for predicting presence/absence of cancer, assuming the antigen is present in blood samples, but that detection of cancer would require otherwise a biopsy. "],
["frischwaughlovell-theorem.html", "3 Frisch–Waugh–Lovell theorem", " 3 Frisch–Waugh–Lovell theorem This result dates back to the work of Frisch, R. and F. Waugh (1933) and of M. Lovell (1963). The FWL theorem has two components: it gives a formula for partitioned OLS estimates and shows that residuals from sequential regressions are identical. Consider the following linear regression \\[ {{\\boldsymbol{y}}}= {{\\mathbf{X}}}_1{\\boldsymbol{\\beta}}_1+{{\\mathbf{X}}}_2{\\boldsymbol{\\beta}}_2+ {\\boldsymbol{u}}, \\label{eq1} \\] where the response vector \\({{\\boldsymbol{y}}}\\) is \\(n \\times 1\\), the vector of errors \\({\\boldsymbol{u}}\\) is a realization from a mean zero random variable. The \\(n \\times p\\) full-rank design matrix \\({{\\mathbf{X}}}\\) can be written as the partitioned matrix \\(({{\\mathbf{X}}}_1^\\top, {{\\mathbf{X}}}_2^\\top)^\\top\\) with blocks \\({{\\mathbf{X}}}_1\\), an \\(n \\times p_1\\) matrix, and \\({{\\mathbf{X}}}_2\\), an \\(n \\times p_2\\) matrix. Let \\({\\hat{\\boldsymbol{\\beta}}}_1\\) and \\({\\hat{\\boldsymbol{\\beta}}}_2\\) be the ordinary least square (OLS) parameter estimates from running this regression. Define the orthogonal projection matrix \\({\\mathbf{H}}_{{\\mathbf{X}}}\\) as usual and \\({\\mathbf{H}}_{{{\\mathbf{X}}}_i} = {{\\mathbf{X}}}_i^{\\vphantom{\\top}}({{\\mathbf{X}}}_i^\\top{{\\mathbf{X}}}_i^{\\vphantom{\\top}})^{-1}{{\\mathbf{X}}}_i^\\top\\) for \\(i=1, 2\\). Similarly, define the complementary projection matrices \\({\\mathbf{M}}_{{{\\mathbf{X}}}_1}=\\mathbf{I}_n-{\\mathbf{H}}_{{{\\mathbf{X}}}_1}\\) and \\({\\mathbf{M}}_{{{\\mathbf{X}}}_2}=\\mathbf{I}_n-{\\mathbf{H}}_{{{\\mathbf{X}}}_2}\\). Theorem 3.1 The ordinary least square estimates of \\({\\boldsymbol{\\beta}}_2\\) and the residuals from are identical to those obtained by running the regression \\[ {\\mathbf{M}}_{{{\\mathbf{X}}}_1}{{\\boldsymbol{y}}}= {\\mathbf{M}}_{{{\\mathbf{X}}}_1}{{\\mathbf{X}}}_2{\\boldsymbol{\\beta}}_2 + \\text{residuals}. \\label{eq2} \\ \\] Proof. The easiest proof uses projection matrices, but we demonstrate the result for OLS coefficients directly. Consider an invertible \\(d \\times d\\) matrix \\(\\mathbf{C}\\) and denote its inverse by \\(\\mathbf{D}\\); then \\[ \\begin{pmatrix} \\mathbf{C}_{11} &amp; \\mathbf{C}_{12} \\\\ \\mathbf{C}_{21} &amp;\\mathbf{C}_{22} \\end{pmatrix}\\begin{pmatrix} \\mathbf{D}_{11} &amp; \\mathbf{D}_{12} \\\\ \\mathbf{D}_{21} &amp;\\mathbf{D}_{22} \\end{pmatrix} =\\mathbf{I}_p \\] gives the relationships \\[\\begin{align*} \\mathbf{C}_{11}\\mathbf{D}_{11}+\\mathbf{C}_{12}\\mathbf{D}_{21} &amp;= \\mathbf{I}_{p_1}\\\\ \\mathbf{C}_{11}\\mathbf{D}_{12}+\\mathbf{C}_{12}\\mathbf{D}_{22} &amp;= \\mathbf{O}_{p_1, p_2}\\\\ \\mathbf{C}_{22}\\mathbf{D}_{21}+\\mathbf{C}_{21}\\mathbf{D}_{11} &amp;= \\mathbf{O}_{p_2, p_1}\\\\ \\mathbf{C}_{22}\\mathbf{D}_{22}+\\mathbf{C}_{21}\\mathbf{D}_{12} &amp;= \\mathbf{I}_{p_2}\\\\ \\end{align*}\\] from which we deduce that the so-called Schur complement of \\(\\mathbf{C}_{22}\\) is \\[\\mathbf{C}_{11}+\\mathbf{C}_{12}\\mathbf{C}^{-1}_{22}\\mathbf{C}_{21} = \\mathbf{D}_{11}^{-1}\\] and \\[ -\\mathbf{C}_{22}\\mathbf{C}_{21}(\\mathbf{C}_{11}+\\mathbf{C}_{12}\\mathbf{C}^{-1}_{22}\\mathbf{C}_{21})^{-1} = \\mathbf{D}_{21}. \\] Substituting \\[ \\begin{pmatrix} \\mathbf{C}_{11} &amp; \\mathbf{C}_{12} \\\\ \\mathbf{C}_{21} &amp;\\mathbf{C}_{22} \\end{pmatrix} \\equiv \\begin{pmatrix} \\mathbf{X}_1^\\top\\mathbf{X}_1 &amp; \\mathbf{X}_1^\\top\\mathbf{X}_2\\\\\\mathbf{X}_2^\\top\\mathbf{X}_1 &amp;\\mathbf{X}_2^\\top\\mathbf{X}_2 \\end{pmatrix} \\] and plug-in this result back in the equation for the least squares yields \\[\\begin{align*} \\hat{\\boldsymbol{\\beta}}_1 &amp;= (\\mathbf{D}_{11}\\mathbf{X}_1^\\top + \\mathbf{D}_{12}\\mathbf{X}_2^\\top)\\boldsymbol{y} \\\\&amp;= \\mathbf{D}_{11}( \\mathbf{X}_1^\\top - \\mathbf{C}_{12}\\mathbf{C}_{22}^{-1}\\mathbf{X}_2)\\boldsymbol{y} \\\\&amp;= \\left(\\mathbf{C}_{11}+\\mathbf{C}_{12}\\mathbf{C}^{-1}_{22}\\mathbf{C}_{21}\\right)^{-1} \\mathbf{X}_1^\\top\\mathbf{M}_{\\mathbf{X}_2}\\boldsymbol{y} \\\\&amp;= (\\mathbf{X}_1^\\top\\mathbf{M}_{\\mathbf{X}_2}\\mathbf{X}_1)^{-1}\\mathbf{X}_1^\\top\\mathbf{M}_{\\mathbf{X}_2}\\boldsymbol{y}. \\end{align*}\\] The proof that the residuals are the same is left as an exercise. "],
["geometric-proof-using-pictures.html", "3.1 Geometric proof using pictures", " 3.1 Geometric proof using pictures The Frisch–Waugh–Lovell theorem is perhaps best explained using pictures. Davidson and McKinnon (1993) ``Estimation and Inference in Econometrics’’ monograph offers a fantastic illustration (Fig 1.7), which is reproduced below for convenience. The explanation is my own. Consider for simplicity the bivariate case and two non-parallel vectors \\(\\mathbf{X}_1\\) and \\(\\mathbf{X}_2\\). The yellow line illustrate the usual orthogonal decomposition \\({\\boldsymbol{y}} = {\\mathbf{H}}_{{\\mathbf{X}}}{{\\boldsymbol{y}}}+ {\\mathbf{M}}_{{\\mathbf{X}}}{{\\boldsymbol{y}}}\\), where \\({\\mathbf{H}}_{{\\mathbf{X}}}{{\\boldsymbol{y}}}\\) is the segment \\(OB\\) and \\({\\mathbf{M}}_{{\\mathbf{X}}}{{\\boldsymbol{y}}}\\) is the segment \\(BA\\). A similar decomposition can be performed in terms of the sole \\({\\mathbf{X}}_1\\), namely through \\({\\mathbf{H}}_{{\\mathbf{X}}_1}{{\\boldsymbol{y}}}\\) corresponding to the line segment \\(OC\\) and \\({\\mathbf{M}}_{{\\mathbf{X}}_1}{{\\boldsymbol{y}}}\\) corresponding to \\(CA\\). Figure 3.1: Orthogonal decomposition into residuals and fitted values We may also look at the projection onto \\({\\mathsf{span}}({\\mathbf{X}})\\) of the vector \\({{\\boldsymbol{y}}}\\) and its fitted value. More specifically, consider the decomposition \\({\\mathbf{H}}_{{\\mathbf{X}}}{{\\boldsymbol{y}}}= {\\mathbf{X}}_1{\\hat{\\boldsymbol{\\beta}}}_1 + {\\mathbf{X}}_2{\\hat{\\boldsymbol{\\beta}}}_2\\). This is represented by the parallelogram \\(ODBE\\); the segments \\(OD\\) and \\(EB\\) correspond to the vector \\({\\mathbf{X}}_2{\\hat{\\boldsymbol{\\beta}}}_2\\), while \\(DB\\) and \\(OE\\) are shifted copies of \\({\\mathbf{X}}_1{\\hat{\\boldsymbol{\\beta}}}_1\\). Figure 3.2: Projection onto spane spanned by the regressors The right-angled triangle \\(ABC\\) is orthogonal to \\({\\mathbf{X}}_1\\). We deduce that the segment \\(CB\\) is \\({\\mathbf{M}}_{{\\mathbf{X}}_1}{\\mathbf{H}}_{{\\mathbf{X}}}{{\\boldsymbol{y}}}\\). Figure 3.3: Triangle ABC We can deduce from Figure 3.3 the formula for least squares since \\({\\mathbf{M}}_{{\\mathbf{X}}_1}{\\mathbf{H}}_{{\\mathbf{X}}}{{\\boldsymbol{y}}}= {\\mathbf{M}}_{{\\mathbf{X}}_1}{\\mathbf{X}}_2{\\hat{\\boldsymbol{\\beta}}}_2\\). It is now hopefully clear that \\(BA\\) is the vector \\(\\boldsymbol{e}\\) of residuals from the original regression. "]
]

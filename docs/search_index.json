[
["computational-considerations.html", "2 Computational considerations", " 2 Computational considerations In this tutorial, we will explore some basic R commands and illustrate their use on the Motor Trend Car Road Tests dataset (mtcars). "],
["calculation-of-least-square-estimates.html", "2.1 Calculation of least square estimates", " 2.1 Calculation of least square estimates Consider as usual \\(\\boldsymbol{y}\\) and \\(n\\)-vector of response variables and a full-rank \\(n \\times p\\) design matrix \\(\\mathbf{X}\\). We are interested in finding the ordinary least square coefficient \\(\\hat{\\boldsymbol{\\beta}}\\), the fitted values \\(\\hat{\\boldsymbol{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\) and the residuals \\(\\boldsymbol{e} = \\boldsymbol{y} - \\mathbf{X}\\boldsymbol{\\beta}\\). Whereas orthogonal projection matrices are useful for theoretical derivations, they are not used for computations. Building \\(\\mathbf{H}_{\\mathbf{X}}\\) involves a matrix inversion and the storage of an \\(n \\times n\\) matrix. In Exercise series 2, we looked at two matrix decompositions: a singular value decomposition (SVD) and a QR decomposition. These are more numerically stable than using the normal equations \\((\\mathbf{X}^\\top\\mathbf{X})\\boldsymbol{\\beta} = \\mathbf{X}^\\top\\boldsymbol{y}\\) (the condition number of the matrix \\(\\mathbf{X}^\\top\\mathbf{X}\\) is the square of that of \\(\\mathbf{X}\\)). For more details about the complexity and algorithms underlying the different methods, the reader is referred to these notes of www.math.uchicago.edu/~may/REU2012/REUPapers/Lee.pdf. 2.1.1 Normal equations The following simply illustrates what has been derived in Exercise series 2. You will probably never use these commands, as R has devoted functions that are coded more efficiently. We can compute first the ordinary least square estimates using the formula \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\boldsymbol{y}\\). data(mtcars) y &lt;- mtcars$mpg X &lt;- cbind(1, as.matrix(mtcars[,2:ncol(mtcars)])) n &lt;- nrow(X) p &lt;- ncol(X) # Estimation of betahat: XtX&lt;- crossprod(X) Xty &lt;- crossprod(X, y) # Solve normal equations betahat &lt;- as.vector(solve(XtX, Xty)) #same as betahat &lt;- solve(t(X) %*% X) %*% t(X) %*% y 2.1.2 Singular value decomposition The SVD decomposition in R returns a list with elements u, d and v. u is the orthonormal \\(n \\times p\\) matrix, d is a vector containing the diagonal elements of \\(\\mathbf{D}\\) and v is the \\(p \\times p\\) orthogonal matrix. Recall that the decomposition is \\[\\mathbf{X} = \\mathbf{UDV}^\\top\\] and that \\(\\mathbf{VV}^\\top= \\mathbf{V}^\\top\\mathbf{V}=\\mathbf{U}^\\top\\mathbf{U}=\\mathbf{I}_p\\). The matrix \\(\\mathbf{D}\\) contains the singular values of \\(\\mathbf{X}\\), and the diagonal elements \\(\\mathrm{d}_{ii}^2\\) corresponds to the (ordered) eigenvalues of \\(\\mathbf{X}^\\top\\mathbf{X}\\). svdX &lt;- svd(X) # Projection matrix Hx &lt;- tcrossprod(svdX$u) # t(U) %*% U gives p by p identity matrix all.equal(crossprod(svdX$u), diag(p)) ## [1] TRUE # V is an orthogonal matrix all.equal(tcrossprod(svdX$v), diag(p)) ## [1] TRUE all.equal(crossprod(svdX$v), diag(p)) ## [1] TRUE # D contains singular values all.equal(svdX$d^2, eigen(XtX, only.values = TRUE)$values) ## [1] TRUE # OLS coefficient from SVD betahat_svd &lt;- c(svdX$v %*% diag(1/svdX$d) %*% t(svdX$u) %*% y) all.equal(betahat_svd, betahat) ## [1] TRUE 2.1.3 QR decomposition R uses a QR-decomposition to calculate the OLS. There are specific functions to return coefficients, fitted values and residuals. One can also obtain the \\(n \\times p\\) matrix \\(\\mathbf{Q}_1\\) and the upper triangular \\(p \\times p\\) matrix \\(\\mathbf{R}\\) from the thinned QR decomposition, \\[\\mathbf{X} = \\mathbf{Q}_1\\mathbf{R}.\\] qrX &lt;- qr(X) Q1 &lt;- qr.Q(qrX) R &lt;- qr.R(qrX) # Compute betahat from QR betahat_qr1 &lt;- qr.coef(qrX, y) #using built-in function betahat_qr2 &lt;- c(backsolve(R, t(Q1) %*% y)) #manually all.equal(betahat, betahat_qr1, check.attributes = FALSE) ## [1] TRUE all.equal(betahat, betahat_qr2, check.attributes = FALSE) ## [1] TRUE # Compute residuals qre &lt;- qr.resid(qrX, y) all.equal(qre, c(y - X %*% betahat), check.attributes = FALSE) ## [1] TRUE # Compute fitted values qryhat &lt;- qr.fitted(qrX, y) all.equal(qryhat, c(X %*% betahat), check.attributes = FALSE) ## [1] TRUE # Compute orthogonal projection matrix qrHx &lt;- tcrossprod(Q1) all.equal(qrHx, Hx) ## [1] TRUE "],
["parameter-estimation.html", "2.2 Parameter estimation", " 2.2 Parameter estimation We are now ready to fit a linear model with an intercept and a linear effect for the weight. The model will be of the form \\[ \\texttt{mpg}_i = \\beta_0 + \\texttt{wt}_i\\beta_1 +\\varepsilon_i.\\] We form the design matrix \\((\\boldsymbol{1}_n^\\top, \\texttt{wt}^\\top)^\\top\\) and the vector of regressand \\(\\texttt{mpg}\\), then proceed with calculating the OLS coefficients \\(\\hat{\\boldsymbol{\\beta}}\\), the hat matrix \\(\\mathbf{H}_{\\mathbf{X}}\\), the fitted values \\(\\hat{\\boldsymbol{y}}\\) and the residuals \\(\\boldsymbol{e}\\). #Design matrix wt &lt;- mtcars$wt X &lt;- cbind(1, wt) mpg &lt;- mtcars$mpg #OLS estimates XtXinv &lt;- solve(crossprod(X)) beta_hat &lt;- c(XtXinv %*% t(X) %*% mpg) #Form orthogonal projection matrix Hmat &lt;- X %*% XtXinv %*% t(X) #Create residuals and fitted values fitted &lt;- Hmat %*% mpg res &lt;- mpg - fitted fitted &lt;- Hmat %*% mpg #Variance estimate and standard errors s2 &lt;- sum(res^2) / (length(res) - ncol(X)) std_err &lt;- sqrt(diag(s2 * XtXinv)) The residuals \\(\\boldsymbol{e} = \\boldsymbol{y} -\\hat{\\boldsymbol{y}}\\) can be interpreted as the vertical distance between the regression slope and the observation. This is illustrated in the following graph. For each observation \\(y_i\\), a vertical line at distance \\(e_i\\) is drawn from the prediction \\(\\hat{y}_i\\). Side remark: graphs and table should always be properly labelled (including units). The last line of the call to plot contains cosmetic options that alter the display of the scatterplot — you can check for yourself the effects of removing any (all) of these additional commands. plot(mpg ~ wt, xlab = &quot;weight (1000 lbs)&quot;, ylab = &quot;Fuel consumption (miles/US gallon)&quot;, main = &quot;Fuel consumption of automobiles, 1974 Motor Trend&quot;, data = mtcars, bty = &quot;l&quot;, pch = 20, ylim = c(0, 35), xlim = c(0, 6)) #options to tweak the display #Line of best linear fit abline(a = beta_hat[1], b = beta_hat[2]) #Residuals are vertical distance from line to for(i in 1:nrow(X)){ segments(x0 = wt[i], y0 = fitted[i], y1 = fitted[i] + res[i], col = 2) } The same scatterplot, this time using ggplot2. library(ggplot2, warn.conflicts = FALSE, quietly = TRUE) #Create data frame with segments vlines &lt;- data.frame(x1 = mtcars$wt, y1 = fitted, y2 = fitted + res) ggg &lt;- ggplot(mtcars, aes(wt, mpg)) + geom_point() + labs(x = &quot;weight (1000 lbs)&quot;, y = &quot;Fuel consumption (miles/US gallon)&quot;, title = &quot;Fuel consumption of automobiles, 1974 Motor Trend&quot;) + geom_segment(aes(x = x1, y = y1, xend = x1, yend = y2, color = &quot;red&quot;), data = vlines, show.legend = FALSE) + geom_abline(slope = beta_hat[2], intercept = beta_hat[1]) print(ggg) "],
["interpretation.html", "2.3 Interpretation", " 2.3 Interpretation If the regression model is \\[y_i = \\beta_0 + \\mathrm{x}_{i1}\\beta_1 + \\mathrm{x}_{i2}\\beta_2 + \\varepsilon_i,\\] the interpretation of \\(\\beta_1\\) in the linear model is as follows: a unit increase in \\(x\\) leads to \\(\\beta_1\\) units increase in \\(y\\), everything else (i.e., \\(\\mathrm{x}_{i2}\\)) being held constant. For the mtcars regression above, an increase of the weight of the car of 1000 pounds leads to an average decrease of 5.34 miles per US gallon in distance covered by the car. We could easily get an equivalent statement in terms of increase of the car fuel consumption for a given distance. "],
["the-lm-function.html", "2.4 The lm function", " 2.4 The lm function The function lm is the workshorse for fitting linear models. It takes as input a formula: suppose you have a data frame containing columns x (a regressor) and y (the regressand); you can then call lm(y ~ x) to fit the linear model \\(y = \\beta_0 + \\beta_1x + \\varepsilon\\). The explanatory variable y is on the left hand side, while the right hand side should contain the predictors, separated by a + sign if there are more than one. If you provide the data frame name using data, then the shorthand y ~ . fits all the columns of the data frame (but y) as regressors. To fit higher order polynomials or transformations, use the I function to tell R to interpret the input “as is”. Thus, lm(y~x+I(x^2)), would fit a linear model with design matrix \\((\\boldsymbol{1}_n, \\mathbf{x}^\\top, \\mathbf{x}^2)^\\top\\). A constant is automatically included in the regression, but can be removed by writing -1 or +0 on the right hand side of the formula. # The function lm and its output fit &lt;- lm(mpg ~ wt, data = mtcars) fit_summary &lt;- summary(fit) The lm output will display OLS estimates along with standard errors, \\(t\\) values for the Wald test of the hypothesis \\(\\mathrm{H}_0: \\beta_i=0\\) and the associated \\(P\\)-values. Other statistics and information about the sample size, the degrees of freedom, etc., are given at the bottom of the table. Many methods allow you to extract specific objects. For example, the functions coef, resid, fitted, model.matrix will return \\(\\hat{\\boldsymbol{\\beta}}\\), \\(\\boldsymbol{e}\\), \\(\\hat{\\boldsymbol{y}}\\) and \\(\\mathbf{X}\\), respectively. names(fit) ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; ## [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; ## [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; names(fit_summary) ## [1] &quot;call&quot; &quot;terms&quot; &quot;residuals&quot; &quot;coefficients&quot; ## [5] &quot;aliased&quot; &quot;sigma&quot; &quot;df&quot; &quot;r.squared&quot; ## [9] &quot;adj.r.squared&quot; &quot;fstatistic&quot; &quot;cov.unscaled&quot; "]
]

<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>lineaRmodels</title>
  <meta name="description" content="This is a web complement to MATH 341 (Linear Models), a first regression course for EPFL mathematicians.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="lineaRmodels" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a web complement to MATH 341 (Linear Models), a first regression course for EPFL mathematicians." />
  <meta name="github-repo" content="lbelzile/lineaRmodels" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="lineaRmodels" />
  
  <meta name="twitter:description" content="This is a web complement to MATH 341 (Linear Models), a first regression course for EPFL mathematicians." />
  

<meta name="author" content="LÃ©o Belzile">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="solutions-1.html">
<link rel="next" href="revisiting-the-interpretation-of-the-parameters-of-a-linear-model.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.2/htmlwidgets.js"></script>
<link href="libs/rglwidgetClass-2/rgl.css" rel="stylesheet" />
<script src="libs/rglwidgetClass-2/rglClass.src.js"></script>
<script src="libs/CanvasMatrix4-2016/CanvasMatrix.src.js"></script>
<script src="libs/rglWebGL-binding-0.100.1/rglWebGL.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  { background-color: #f8f8f8; }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Linear Models</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preliminary remarks</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="basics-of-r.html"><a href="basics-of-r.html"><i class="fa fa-check"></i><b>1.1</b> Basics of <strong>R</strong></a><ul>
<li class="chapter" data-level="1.1.1" data-path="basics-of-r.html"><a href="basics-of-r.html#help"><i class="fa fa-check"></i><b>1.1.1</b> Help</a></li>
<li class="chapter" data-level="1.1.2" data-path="basics-of-r.html"><a href="basics-of-r.html#basic-commands"><i class="fa fa-check"></i><b>1.1.2</b> Basic commands</a></li>
<li class="chapter" data-level="1.1.3" data-path="basics-of-r.html"><a href="basics-of-r.html#linear-algebra-in-r"><i class="fa fa-check"></i><b>1.1.3</b> Linear algebra in <strong>R</strong></a></li>
<li class="chapter" data-level="1.1.4" data-path="basics-of-r.html"><a href="basics-of-r.html#packages"><i class="fa fa-check"></i><b>1.1.4</b> Packages</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="week1.html"><a href="week1.html"><i class="fa fa-check"></i><b>1.2</b> Tutorial 1</a><ul>
<li class="chapter" data-level="1.2.1" data-path="week1.html"><a href="week1.html#datasets"><i class="fa fa-check"></i><b>1.2.1</b> Datasets</a></li>
<li class="chapter" data-level="1.2.2" data-path="week1.html"><a href="week1.html#graphics"><i class="fa fa-check"></i><b>1.2.2</b> Graphics</a></li>
<li class="chapter" data-level="1.2.3" data-path="week1.html"><a href="week1.html#projection-matrices"><i class="fa fa-check"></i><b>1.2.3</b> Projection matrices</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.3</b> Exercises</a><ul>
<li class="chapter" data-level="1.3.1" data-path="exercises.html"><a href="exercises.html#auto-dataset"><i class="fa fa-check"></i><b>1.3.1</b> Auto dataset</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="solutions.html"><a href="solutions.html"><i class="fa fa-check"></i><b>1.4</b> Solutions</a><ul>
<li class="chapter" data-level="1.4.1" data-path="solutions.html"><a href="solutions.html#exercise-1.4---oblique-projections"><i class="fa fa-check"></i><b>1.4.1</b> Exercise 1.4 - Oblique projections</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="summary-of-week-1.html"><a href="summary-of-week-1.html"><i class="fa fa-check"></i><b>1.5</b> Summary of week 1</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="computational-considerations.html"><a href="computational-considerations.html"><i class="fa fa-check"></i><b>2</b> Computational considerations</a><ul>
<li class="chapter" data-level="2.1" data-path="calculation-of-least-square-estimates.html"><a href="calculation-of-least-square-estimates.html"><i class="fa fa-check"></i><b>2.1</b> Calculation of least square estimates</a></li>
<li class="chapter" data-level="2.2" data-path="interpretation-of-the-coefficients.html"><a href="interpretation-of-the-coefficients.html"><i class="fa fa-check"></i><b>2.2</b> Interpretation of the coefficients</a></li>
<li class="chapter" data-level="2.3" data-path="the-lm-function.html"><a href="the-lm-function.html"><i class="fa fa-check"></i><b>2.3</b> The <code>lm</code> function</a><ul>
<li class="chapter" data-level="2.3.1" data-path="the-lm-function.html"><a href="the-lm-function.html#singular-value-decomposition"><i class="fa fa-check"></i><b>2.3.1</b> Singular value decomposition</a></li>
<li class="chapter" data-level="2.3.2" data-path="the-lm-function.html"><a href="the-lm-function.html#qr-decomposition"><i class="fa fa-check"></i><b>2.3.2</b> QR decomposition</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="parameter-estimation.html"><a href="parameter-estimation.html"><i class="fa fa-check"></i><b>2.4</b> Parameter estimation</a></li>
<li class="chapter" data-level="2.5" data-path="the-hyperplane-of-fitted-values.html"><a href="the-hyperplane-of-fitted-values.html"><i class="fa fa-check"></i><b>2.5</b> The hyperplane of fitted values</a></li>
<li class="chapter" data-level="2.6" data-path="centered-coefficient-of-determination.html"><a href="centered-coefficient-of-determination.html"><i class="fa fa-check"></i><b>2.6</b> (Centered) coefficient of determination</a></li>
<li class="chapter" data-level="2.7" data-path="summary-of-week-2.html"><a href="summary-of-week-2.html"><i class="fa fa-check"></i><b>2.7</b> Summary of week 2</a></li>
<li class="chapter" data-level="2.8" data-path="solutions-1.html"><a href="solutions-1.html"><i class="fa fa-check"></i><b>2.8</b> Solutions</a><ul>
<li class="chapter" data-level="2.8.1" data-path="solutions-1.html"><a href="solutions-1.html#exercise-3.5---prostate-cancer"><i class="fa fa-check"></i><b>2.8.1</b> Exercise 3.5 - Prostate cancer</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="frischwaughlovell-theorem.html"><a href="frischwaughlovell-theorem.html"><i class="fa fa-check"></i><b>3</b> FrischâWaughâLovell theorem</a><ul>
<li class="chapter" data-level="3.1" data-path="revisiting-the-interpretation-of-the-parameters-of-a-linear-model.html"><a href="revisiting-the-interpretation-of-the-parameters-of-a-linear-model.html"><i class="fa fa-check"></i><b>3.1</b> Revisiting the interpretation of the parameters of a linear model</a></li>
<li class="chapter" data-level="3.2" data-path="factors.html"><a href="factors.html"><i class="fa fa-check"></i><b>3.2</b> Factors</a></li>
<li class="chapter" data-level="3.3" data-path="example-seasonal-effects.html"><a href="example-seasonal-effects.html"><i class="fa fa-check"></i><b>3.3</b> Example: seasonal effects</a></li>
<li class="chapter" data-level="3.4" data-path="solutions-2.html"><a href="solutions-2.html"><i class="fa fa-check"></i><b>3.4</b> Solutions</a><ul>
<li class="chapter" data-level="3.4.1" data-path="solutions-2.html"><a href="solutions-2.html#exercise-4.4"><i class="fa fa-check"></i><b>3.4.1</b> Exercise 4.4</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="gaussian-linear-model.html"><a href="gaussian-linear-model.html"><i class="fa fa-check"></i><b>4</b> Gaussian linear model</a><ul>
<li class="chapter" data-level="4.1" data-path="confidence-and-prediction-intervals.html"><a href="confidence-and-prediction-intervals.html"><i class="fa fa-check"></i><b>4.1</b> Confidence and prediction intervals</a></li>
<li class="chapter" data-level="4.2" data-path="residuals.html"><a href="residuals.html"><i class="fa fa-check"></i><b>4.2</b> Residuals</a></li>
<li class="chapter" data-level="4.3" data-path="diagnostic-plots.html"><a href="diagnostic-plots.html"><i class="fa fa-check"></i><b>4.3</b> Diagnostic plots</a><ul>
<li class="chapter" data-level="4.3.1" data-path="diagnostic-plots.html"><a href="diagnostic-plots.html#added-variable-plots"><i class="fa fa-check"></i><b>4.3.1</b> Added-variable plots</a></li>
<li class="chapter" data-level="4.3.2" data-path="diagnostic-plots.html"><a href="diagnostic-plots.html#diagnostic-of-heteroscedasticity"><i class="fa fa-check"></i><b>4.3.2</b> Diagnostic of heteroscedasticity</a></li>
<li class="chapter" data-level="4.3.3" data-path="diagnostic-plots.html"><a href="diagnostic-plots.html#outliers"><i class="fa fa-check"></i><b>4.3.3</b> Outliers</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="qqplot.html"><a href="qqplot.html"><i class="fa fa-check"></i><b>4.4</b> Quantile-quantile plots</a><ul>
<li class="chapter" data-level="4.4.1" data-path="qqplot.html"><a href="qqplot.html#quantile-quantile-plot-of-externally-studentized-errors"><i class="fa fa-check"></i><b>4.4.1</b> Quantile-quantile plot of externally studentized errors</a></li>
<li class="chapter" data-level="4.4.2" data-path="qqplot.html"><a href="qqplot.html#quantile-quantile-plot-using-the-qr-decomposition"><i class="fa fa-check"></i><b>4.4.2</b> Quantile-quantile plot using the QR decomposition</a></li>
<li class="chapter" data-level="4.4.3" data-path="qqplot.html"><a href="qqplot.html#monte-carlo-methods-for-confidence-intervals"><i class="fa fa-check"></i><b>4.4.3</b> Monte Carlo methods for confidence intervals</a></li>
<li class="chapter" data-level="4.4.4" data-path="qqplot.html"><a href="qqplot.html#parametric-bootstrap-confidence-intervals-using-the-qr-decomposition"><i class="fa fa-check"></i><b>4.4.4</b> Parametric bootstrap confidence intervals using the QR decomposition</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="solutions-3.html"><a href="solutions-3.html"><i class="fa fa-check"></i><b>4.5</b> Solutions</a><ul>
<li class="chapter" data-level="4.5.1" data-path="solutions-3.html"><a href="solutions-3.html#exercise-7.1---study-of-growth-hormones"><i class="fa fa-check"></i><b>4.5.1</b> Exercise 7.1 - Study of growth hormones</a></li>
<li class="chapter" data-level="4.5.2" data-path="solutions-3.html"><a href="solutions-3.html#exercise-7.2---electric-production-of-windmills"><i class="fa fa-check"></i><b>4.5.2</b> Exercise 7.2 - Electric production of windmills</a></li>
<li class="chapter" data-level="4.5.3" data-path="solutions-3.html"><a href="solutions-3.html#exercise-7.3---air-traffic"><i class="fa fa-check"></i><b>4.5.3</b> Exercise 7.3 - Air traffic</a></li>
<li class="chapter" data-level="4.5.4" data-path="solutions-3.html"><a href="solutions-3.html#exercise-7.4---determinants-of-earnings"><i class="fa fa-check"></i><b>4.5.4</b> Exercise 7.4 - Determinants of earnings</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="analysis-of-variance.html"><a href="analysis-of-variance.html"><i class="fa fa-check"></i><b>5</b> Analysis of variance</a><ul>
<li class="chapter" data-level="5.1" data-path="sum-of-squares-decomposition.html"><a href="sum-of-squares-decomposition.html"><i class="fa fa-check"></i><b>5.1</b> Sum of squares decomposition</a><ul>
<li class="chapter" data-level="5.1.1" data-path="sum-of-squares-decomposition.html"><a href="sum-of-squares-decomposition.html#the-decomposition-of-squares-in-r"><i class="fa fa-check"></i><b>5.1.1</b> The decomposition of squares in <strong>R</strong></a></li>
<li class="chapter" data-level="5.1.2" data-path="sum-of-squares-decomposition.html"><a href="sum-of-squares-decomposition.html#dropping-or-adding-variables"><i class="fa fa-check"></i><b>5.1.2</b> Dropping or adding variables</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="one-way-anova.html"><a href="one-way-anova.html"><i class="fa fa-check"></i><b>5.2</b> One-way ANOVA</a></li>
<li class="chapter" data-level="5.3" data-path="two-way-anova-and-irrelevant-hypotheses.html"><a href="two-way-anova-and-irrelevant-hypotheses.html"><i class="fa fa-check"></i><b>5.3</b> Two-way ANOVA and irrelevant hypotheses</a></li>
<li class="chapter" data-level="5.4" data-path="solutions-4.html"><a href="solutions-4.html"><i class="fa fa-check"></i><b>5.4</b> Solutions</a><ul>
<li class="chapter" data-level="5.4.1" data-path="solutions-4.html"><a href="solutions-4.html#exercise-9.3---one-way-analysis-of-variance"><i class="fa fa-check"></i><b>5.4.1</b> Exercise 9.3 - One-way analysis of variance</a></li>
<li class="chapter" data-level="5.4.2" data-path="solutions-4.html"><a href="solutions-4.html#exercise-9.4---two-way-analysis-of-variance"><i class="fa fa-check"></i><b>5.4.2</b> Exercise 9.4 - Two-way analysis of variance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>6</b> Hypothesis testing</a></li>
<li class="chapter" data-level="7" data-path="model-selection.html"><a href="model-selection.html"><i class="fa fa-check"></i><b>7</b> Model selection</a><ul>
<li class="chapter" data-level="7.1" data-path="example-price-of-diamonds.html"><a href="example-price-of-diamonds.html"><i class="fa fa-check"></i><b>7.1</b> Example: Price of diamonds</a><ul>
<li class="chapter" data-level="7.1.1" data-path="example-price-of-diamonds.html"><a href="example-price-of-diamonds.html#exploratory-data-analysis"><i class="fa fa-check"></i><b>7.1.1</b> Exploratory data analysis</a></li>
<li class="chapter" data-level="7.1.2" data-path="example-price-of-diamonds.html"><a href="example-price-of-diamonds.html#model-selection-1"><i class="fa fa-check"></i><b>7.1.2</b> Model selection</a></li>
<li class="chapter" data-level="7.1.3" data-path="example-price-of-diamonds.html"><a href="example-price-of-diamonds.html#information-criterion"><i class="fa fa-check"></i><b>7.1.3</b> Information criterion</a></li>
<li class="chapter" data-level="7.1.4" data-path="example-price-of-diamonds.html"><a href="example-price-of-diamonds.html#cross-validation"><i class="fa fa-check"></i><b>7.1.4</b> Cross-validation</a></li>
<li class="chapter" data-level="7.1.5" data-path="example-price-of-diamonds.html"><a href="example-price-of-diamonds.html#presentation-of-results"><i class="fa fa-check"></i><b>7.1.5</b> Presentation of results</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="model-selection-invalidates-p-values.html"><a href="model-selection-invalidates-p-values.html"><i class="fa fa-check"></i><b>7.2</b> Model selection invalidates P-values</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">lineaRmodels</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="frischwaughlovell-theorem" class="section level1">
<h1><span class="header-section-number">3</span> FrischâWaughâLovell theorem</h1>
<p>The FWL theorem has two components: it gives a formula for partitioned OLS estimates and shows that residuals from sequential regressions are identical.</p>
<p>Consider the following linear regression
<span class="math display">\[
 {\boldsymbol{y}}= \mathbf{X}_1\boldsymbol{\beta}_1+\mathbf{X}_2\boldsymbol{\beta}_2+ \boldsymbol{u}, \label{eq1}
\]</span>
where the response vector <span class="math inline">\({\boldsymbol{y}}\)</span> is <span class="math inline">\(n \times 1\)</span>, the vector of errors <span class="math inline">\(\boldsymbol{u}\)</span> is a realization from a mean zero random
variable. The <span class="math inline">\(n \times p\)</span> full-rank design matrix <span class="math inline">\(\mathbf{X}\)</span> can be written as the partitioned
matrix <span class="math inline">\((\mathbf{X}_1^\top, \mathbf{X}_2^\top)^\top\)</span> with blocks <span class="math inline">\(\mathbf{X}_1\)</span>, an <span class="math inline">\(n \times p_1\)</span> matrix, and <span class="math inline">\(\mathbf{X}_2\)</span>, an <span class="math inline">\(n \times p_2\)</span> matrix. Let
<span class="math inline">\(\hat{\boldsymbol{\beta}}_1\)</span>
and <span class="math inline">\(\hat{\boldsymbol{\beta}}_2\)</span> be the ordinary
least square (OLS) parameter estimates from running this regression. Define the orthogonal projection matrix <span class="math inline">\(\mathbf{H}_\mathbf{X}\)</span> as usual and
<span class="math inline">\(\mathbf{H}_{\mathbf{X}_i} = \mathbf{X}_i(\mathbf{X}_i^\top\mathbf{X}_i)^{-1}\mathbf{X}_i^\top\)</span> for <span class="math inline">\(i=1, 2\)</span>. Similarly,
define the complementary projection matrices <span class="math inline">\(\mathbf{M}_{\mathbf{X}_1}=\mathbf{I}_n-\mathbf{H}_{\mathbf{X}_1}\)</span> and <span class="math inline">\(\mathbf{M}_{\mathbf{X}_2}=\mathbf{I}_n-\mathbf{H}_{\mathbf{X}_2}\)</span>.</p>

<div class="theorem">
<span id="thm:unnamed-chunk-13" class="theorem"><strong>Theorem 3.1  </strong></span>The ordinary least square estimates of <span class="math inline">\(\boldsymbol{\beta}_2\)</span> and the residuals from  are identical to those obtained by
running the regression
<span class="math display">\[
 \mathbf{M}_{\mathbf{X}_1}{\boldsymbol{y}}= \mathbf{M}_{\mathbf{X}_1}\mathbf{X}_2\boldsymbol{\beta}_2 + \text{residuals}. \label{eq2} \
\]</span>
</div>


<div class="rmdcaution">
In general, premultiplying both sides of the regression model by a projection matrix alters the model, so you will get different fitted values and residuals. Similarly, the model
<span class="math display">\[\boldsymbol{Y} = \mathbf{X}_1 \boldsymbol{\beta}_1 + \mathbf{X}_2\boldsymbol{\beta}_2 + \boldsymbol{\varepsilon}\]</span>
is <strong>not</strong> equivalent to
<span class="math display">\[
\mathbf{M}_{\mathbf{X}_1}\boldsymbol{y} = \mathbf{M}_{\mathbf{X}_1}\mathbf{X}_2 \boldsymbol{\beta}_2 + \mathbf{M}_{\mathbf{X}_1}\boldsymbol{\varepsilon}
\]</span>
because <span class="math inline">\(\boldsymbol{\varepsilon}\)</span> is not in <span class="math inline">\(\mathbf{M}_\mathbf{X}\)</span>. This is true for the orthogonal decomposition
<span class="math display">\[
\mathbf{M}_{\mathbf{X}_1}\boldsymbol{y} = \mathbf{M}_{\mathbf{X}_1}\mathbf{X}_2 \hat{\boldsymbol{\beta}}_2 + \mathbf{M}_{\mathbf{X}_1}\boldsymbol{e}
\]</span>
</div>

<p>Below is an algebraic proof of the equality of the OLS coefficients. The following material is <strong>optional</strong>.</p>

<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> The easiest proof uses projection matrices, but we demonstrate the result for OLS coefficients directly.
Consider an invertible <span class="math inline">\(d \times d\)</span> matrix <span class="math inline">\(\mathbf{C}\)</span> and denote its inverse by <span class="math inline">\(\mathbf{D}\)</span>; then
<span class="math display">\[
\begin{pmatrix} \mathbf{C}_{11} &amp; \mathbf{C}_{12} \\ \mathbf{C}_{21} &amp;\mathbf{C}_{22}
\end{pmatrix}\begin{pmatrix} \mathbf{D}_{11} &amp; \mathbf{D}_{12} \\ \mathbf{D}_{21} &amp;\mathbf{D}_{22}
\end{pmatrix}
=\mathbf{I}_p
\]</span>
gives the relationships
<span class="math display">\[\begin{align*}
\mathbf{C}_{11}\mathbf{D}_{11}+\mathbf{C}_{12}\mathbf{D}_{21} &amp;= \mathbf{I}_{p_1}\\
\mathbf{C}_{11}\mathbf{D}_{12}+\mathbf{C}_{12}\mathbf{D}_{22} &amp;= \mathbf{O}_{p_1, p_2}\\
\mathbf{C}_{22}\mathbf{D}_{21}+\mathbf{C}_{21}\mathbf{D}_{11} &amp;= \mathbf{O}_{p_2, p_1}\\
\mathbf{C}_{22}\mathbf{D}_{22}+\mathbf{C}_{21}\mathbf{D}_{12} &amp;= \mathbf{I}_{p_2}\\
\end{align*}\]</span>
from which we deduce that the so-called Schur complement of <span class="math inline">\(\mathbf{C}_{22}\)</span> is <span class="math display">\[\mathbf{C}_{11}+\mathbf{C}_{12}\mathbf{C}^{-1}_{22}\mathbf{C}_{21} = \mathbf{D}_{11}^{-1}\]</span>
and
<span class="math display">\[
-\mathbf{C}_{22}\mathbf{C}_{21}(\mathbf{C}_{11}+\mathbf{C}_{12}\mathbf{C}^{-1}_{22}\mathbf{C}_{21})^{-1} = \mathbf{D}_{21}.
\]</span>
Substituting
<span class="math display">\[
\begin{pmatrix} \mathbf{C}_{11} &amp; \mathbf{C}_{12} \\ \mathbf{C}_{21} &amp;\mathbf{C}_{22}
\end{pmatrix} \equiv \begin{pmatrix} \mathbf{X}_1^\top\mathbf{X}_1 &amp; \mathbf{X}_1^\top\mathbf{X}_2\\\mathbf{X}_2^\top\mathbf{X}_1  &amp;\mathbf{X}_2^\top\mathbf{X}_2 
\end{pmatrix}
\]</span>
and plug-in this result back in the equation for the least squares yields
<span class="math display">\[\begin{align*}
\hat{\boldsymbol{\beta}}_1 &amp;= (\mathbf{D}_{11}\mathbf{X}_1^\top + \mathbf{D}_{12}\mathbf{X}_2^\top)\boldsymbol{y} 
\\&amp;= \mathbf{D}_{11}( \mathbf{X}_1^\top - \mathbf{C}_{12}\mathbf{C}_{22}^{-1}\mathbf{X}_2)\boldsymbol{y}
\\&amp;= \left(\mathbf{C}_{11}+\mathbf{C}_{12}\mathbf{C}^{-1}_{22}\mathbf{C}_{21}\right)^{-1} \mathbf{X}_1^\top\mathbf{M}_{\mathbf{X}_2}\boldsymbol{y} 
\\&amp;= (\mathbf{X}_1^\top\mathbf{M}_{\mathbf{X}_2}\mathbf{X}_1)^{-1}\mathbf{X}_1^\top\mathbf{M}_{\mathbf{X}_2}\boldsymbol{y}.
\end{align*}\]</span></p>
The proof that the residuals are the same is left as an exercise.
</div>

<p>The FrischâWaughâLovell theorem dates back to the work of <a href="https://www.jstor.org/stable/1907330">Frisch, R. and F. Waugh (1933)</a> and of <a href="https://doi.org/10.1080/01621459.1963.10480682">M. Lovell (1963)</a>.</p> 
</div>
            </section>

          </div>
        </div>
      </div>
<a href="solutions-1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="revisiting-the-interpretation-of-the-parameters-of-a-linear-model.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["LinesRModels.pdf", "LinesRModels.epub"],
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
